{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "uuid": "bdb8d957-bfe5-4f35-9e17-20ee37c197ed"
   },
   "outputs": [],
   "source": [
    "# 导入库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from IPython.display import display\n",
    "import pandas_profiling\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "bdb8d957-bfe5-4f35-9e17-20ee37c197ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taili\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3051: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>5263</td>\n",
       "      <td>555</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>6064</td>\n",
       "      <td>625</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>8314</td>\n",
       "      <td>821</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>13995</td>\n",
       "      <td>1498</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>4822</td>\n",
       "      <td>559</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  DayOfWeek        Date  Sales  Customers  Open  Promo StateHoliday  \\\n",
       "0      1          5  2015-07-31   5263        555     1      1            0   \n",
       "1      2          5  2015-07-31   6064        625     1      1            0   \n",
       "2      3          5  2015-07-31   8314        821     1      1            0   \n",
       "3      4          5  2015-07-31  13995       1498     1      1            0   \n",
       "4      5          5  2015-07-31   4822        559     1      1            0   \n",
       "\n",
       "   SchoolHoliday  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 导入数据\n",
    "data_train = pd.read_csv(\"train.csv\")\n",
    "data_store = pd.read_csv(\"store.csv\")\n",
    "data_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# 显示第一条记录\n",
    "display(data_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#交易数据可视化\n",
    "#data_train.profile_report(style={'full_width':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#商店数据可视化\n",
    "#data_store.profile_report(style={'full_width':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "uuid": "fa14f835-7043-4f9e-a537-198661ea8b6f"
   },
   "outputs": [],
   "source": [
    "#Null值处理\n",
    "data_store.fillna(0,inplace=True)\n",
    "data_test.fillna(1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store            0\n",
       "DayOfWeek        0\n",
       "Date             0\n",
       "Sales            0\n",
       "Customers        0\n",
       "Open             0\n",
       "Promo            0\n",
       "StateHoliday     0\n",
       "SchoolHoliday    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Store            0\n",
       "DayOfWeek        0\n",
       "Date             0\n",
       "Sales            0\n",
       "Customers        0\n",
       "Open             0\n",
       "Promo            0\n",
       "StateHoliday     0\n",
       "SchoolHoliday    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Store                        0\n",
       "StoreType                    0\n",
       "Assortment                   0\n",
       "CompetitionDistance          0\n",
       "CompetitionOpenSinceMonth    0\n",
       "CompetitionOpenSinceYear     0\n",
       "Promo2                       0\n",
       "Promo2SinceWeek              0\n",
       "Promo2SinceYear              0\n",
       "PromoInterval                0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#确定Null值处理完毕\n",
    "display(data_train.isnull().sum(),data_train.isnull().sum(),data_store.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store             int64\n",
       "DayOfWeek         int64\n",
       "Date             object\n",
       "Sales             int64\n",
       "Customers         int64\n",
       "Open              int64\n",
       "Promo             int64\n",
       "StateHoliday     object\n",
       "SchoolHoliday     int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Id                 int64\n",
       "Store              int64\n",
       "DayOfWeek          int64\n",
       "Date              object\n",
       "Open             float64\n",
       "Promo              int64\n",
       "StateHoliday      object\n",
       "SchoolHoliday      int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#查看字段类型，确定字段类型能够导入模型\n",
    "display(data_train.dtypes,data_test.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#日期格式转换\n",
    "data_train['Date'] = pd.to_datetime(data_train['Date'])\n",
    "data_test['Date'] = pd.to_datetime(data_test['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#交易数据集与商店数据集合并\n",
    "train = pd.merge(data_train, data_store, on='Store')\n",
    "test = pd.merge(data_test, data_store, on='Store')\n",
    "train = train.sort_values(['Date'],ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#由于本项目是预测未来6周销售额，因此在拆分验证集时，也按未来6周进行拆分\n",
    "#交易数据集中共有1115家商店，按照日期排序后，1115家未来6周的销售数据应为最后1115*7*6行\n",
    "#训练集与测试集切片\n",
    "#参考https://www.kaggle.com/c/rossmann-store-sales/discussion/18024\n",
    "\n",
    "tai_test = train[:6*7*1115]\n",
    "tai_train = train[6*7*1115:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "uuid": "1be9958e-cffb-4179-9097-d013498a2e6d"
   },
   "outputs": [],
   "source": [
    "#未营业的商店不产生销量，属于噪音数据，应剔除\n",
    "tai_test = tai_test[(tai_test[\"Open\"] != 0)&(tai_test[\"Sales\"] > 0)]\n",
    "tai_train = tai_train[(tai_train[\"Open\"] != 0)&(tai_train[\"Sales\"] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建特征处理函数\n",
    "\n",
    "def features_handle(data):\n",
    "    data['Year'] = data.Date.dt.year\n",
    "    data['Month'] = data.Date.dt.month\n",
    "    data['Day'] = data.Date.dt.day\n",
    "    data['DayOfWeek'] = data.Date.dt.dayofweek\n",
    "    data['WeekOfYear'] = data.Date.dt.weekofyear\n",
    "    \n",
    "    mappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\n",
    "    \n",
    "    data.StoreType.replace(mappings, inplace=True)\n",
    "    data.Assortment.replace(mappings, inplace=True)\n",
    "    data.StateHoliday.replace(mappings, inplace=True)\n",
    "    \n",
    "    #替换完成后进行类型转化\n",
    "    data['StateHoliday'] = data['StateHoliday'].astype('int')\n",
    "    data['Assortment'] = data['Assortment'].astype('int')\n",
    "    data['StoreType'] = data['StoreType'].astype('int')\n",
    "    \n",
    "    data['CompetitionOpen'] = 12 * (data.Year - data.CompetitionOpenSinceYear) + (data.Month - data.CompetitionOpenSinceMonth)\n",
    "    data['PromoOpen'] = 12 * (data.Year - data.Promo2SinceYear) + (data.WeekOfYear - data.Promo2SinceWeek) / 4.0\n",
    "    data['CompetitionOpen'] = data.CompetitionOpen.apply(lambda x: x if x > 0 else 0)        \n",
    "    data['PromoOpen'] = data.PromoOpen.apply(lambda x: x if x > 0 else 0)\n",
    "  \n",
    "    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun',7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "    data['month_str'] = data.Month.map(month2str)\n",
    "    def check(row):\n",
    "        if isinstance(row['PromoInterval'],str) and row['month_str'] in row['PromoInterval']:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    data['IsPromoMonth'] =  data.apply(lambda row: check(row),axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>Assortment</th>\n",
       "      <th>...</th>\n",
       "      <th>Promo2SinceYear</th>\n",
       "      <th>PromoInterval</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>WeekOfYear</th>\n",
       "      <th>CompetitionOpen</th>\n",
       "      <th>PromoOpen</th>\n",
       "      <th>month_str</th>\n",
       "      <th>IsPromoMonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>38</td>\n",
       "      <td>84.0</td>\n",
       "      <td>24189.50</td>\n",
       "      <td>Sept</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>857</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-09-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>38</td>\n",
       "      <td>84.0</td>\n",
       "      <td>24189.50</td>\n",
       "      <td>Sept</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1713</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-09-15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>84.0</td>\n",
       "      <td>24189.50</td>\n",
       "      <td>Sept</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2569</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-09-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>84.0</td>\n",
       "      <td>24189.50</td>\n",
       "      <td>Sept</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3425</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2015-09-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>37</td>\n",
       "      <td>84.0</td>\n",
       "      <td>24189.25</td>\n",
       "      <td>Sept</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41083</th>\n",
       "      <td>37664</td>\n",
       "      <td>1115</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-08-05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Mar,Jun,Sept,Dec</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>24188.0</td>\n",
       "      <td>38.50</td>\n",
       "      <td>Aug</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41084</th>\n",
       "      <td>38520</td>\n",
       "      <td>1115</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-08-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Mar,Jun,Sept,Dec</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>24188.0</td>\n",
       "      <td>38.50</td>\n",
       "      <td>Aug</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41085</th>\n",
       "      <td>39376</td>\n",
       "      <td>1115</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-08-03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Mar,Jun,Sept,Dec</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>24188.0</td>\n",
       "      <td>38.50</td>\n",
       "      <td>Aug</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41086</th>\n",
       "      <td>40232</td>\n",
       "      <td>1115</td>\n",
       "      <td>6</td>\n",
       "      <td>2015-08-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Mar,Jun,Sept,Dec</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>24188.0</td>\n",
       "      <td>38.25</td>\n",
       "      <td>Aug</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41087</th>\n",
       "      <td>41088</td>\n",
       "      <td>1115</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-08-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Mar,Jun,Sept,Dec</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>24188.0</td>\n",
       "      <td>38.25</td>\n",
       "      <td>Aug</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41088 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id  Store  DayOfWeek       Date  Open  Promo  StateHoliday  \\\n",
       "0          1      1          3 2015-09-17   1.0      1             0   \n",
       "1        857      1          2 2015-09-16   1.0      1             0   \n",
       "2       1713      1          1 2015-09-15   1.0      1             0   \n",
       "3       2569      1          0 2015-09-14   1.0      1             0   \n",
       "4       3425      1          6 2015-09-13   0.0      0             0   \n",
       "...      ...    ...        ...        ...   ...    ...           ...   \n",
       "41083  37664   1115          2 2015-08-05   1.0      1             0   \n",
       "41084  38520   1115          1 2015-08-04   1.0      1             0   \n",
       "41085  39376   1115          0 2015-08-03   1.0      1             0   \n",
       "41086  40232   1115          6 2015-08-02   0.0      0             0   \n",
       "41087  41088   1115          5 2015-08-01   1.0      0             0   \n",
       "\n",
       "       SchoolHoliday  StoreType  Assortment  ...  Promo2SinceYear  \\\n",
       "0                  0          3           1  ...              0.0   \n",
       "1                  0          3           1  ...              0.0   \n",
       "2                  0          3           1  ...              0.0   \n",
       "3                  0          3           1  ...              0.0   \n",
       "4                  0          3           1  ...              0.0   \n",
       "...              ...        ...         ...  ...              ...   \n",
       "41083              1          4           3  ...           2012.0   \n",
       "41084              1          4           3  ...           2012.0   \n",
       "41085              1          4           3  ...           2012.0   \n",
       "41086              1          4           3  ...           2012.0   \n",
       "41087              1          4           3  ...           2012.0   \n",
       "\n",
       "          PromoInterval  Year  Month  Day  WeekOfYear CompetitionOpen  \\\n",
       "0                     0  2015      9   17          38            84.0   \n",
       "1                     0  2015      9   16          38            84.0   \n",
       "2                     0  2015      9   15          38            84.0   \n",
       "3                     0  2015      9   14          38            84.0   \n",
       "4                     0  2015      9   13          37            84.0   \n",
       "...                 ...   ...    ...  ...         ...             ...   \n",
       "41083  Mar,Jun,Sept,Dec  2015      8    5          32         24188.0   \n",
       "41084  Mar,Jun,Sept,Dec  2015      8    4          32         24188.0   \n",
       "41085  Mar,Jun,Sept,Dec  2015      8    3          32         24188.0   \n",
       "41086  Mar,Jun,Sept,Dec  2015      8    2          31         24188.0   \n",
       "41087  Mar,Jun,Sept,Dec  2015      8    1          31         24188.0   \n",
       "\n",
       "       PromoOpen  month_str  IsPromoMonth  \n",
       "0       24189.50       Sept             0  \n",
       "1       24189.50       Sept             0  \n",
       "2       24189.50       Sept             0  \n",
       "3       24189.50       Sept             0  \n",
       "4       24189.25       Sept             0  \n",
       "...          ...        ...           ...  \n",
       "41083      38.50        Aug             0  \n",
       "41084      38.50        Aug             0  \n",
       "41085      38.50        Aug             0  \n",
       "41086      38.25        Aug             0  \n",
       "41087      38.25        Aug             0  \n",
       "\n",
       "[41088 rows x 25 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#特征批量处理\n",
    "features_handle(tai_train)\n",
    "features_handle(tai_test)\n",
    "features_handle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#丢弃不需要特征\n",
    "drop_list = ['Date','Customers','Open','PromoInterval','month_str']\n",
    "tai_train.drop(drop_list,axis=1,inplace =True)\n",
    "tai_test.drop(drop_list,axis=1,inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成XGboost训练集与验证集,将销量转化为正态分布\n",
    "tai_xtrain = tai_train.drop(['Sales'],axis=1 )\n",
    "tai_ytrain = np.log1p(tai_train.Sales)\n",
    "tai_xtest = tai_test.drop(['Sales'],axis=1 )\n",
    "tai_ytest = np.log1p(tai_test.Sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试集丢弃无用信息\n",
    "xtest =test.drop(['Date','Id','Open','PromoInterval','month_str'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义评估函数\n",
    "def rmspe(y_true, y_pred):\n",
    "    loss = np.sqrt(np.mean(np.square(((y_true - y_pred) / y_true)), axis=0))\n",
    "    return loss\n",
    "\n",
    "#XGBoost评估函数输入参数反向\n",
    "def rmspe_xg(y_pred, y_true):\n",
    "    return \"rmspe\", rmspe(np.expm1(y_true.get_label()),np.expm1(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "#XGboost参数设置\n",
    "params = {\"objective\": \"reg:squarederror\",\n",
    "          'learning_rate':0.1,\n",
    "          \"booster\" : \"gbtree\",\n",
    "          \"max_depth\": 10,\n",
    "          \"min_child_weight\":1,\n",
    "          \"subsample\": 0.9,\n",
    "          \"colsample_bytree\": 0.7,\n",
    "          \"gamma\":0.8,\n",
    "          \"silent\": 1,\n",
    "          \"seed\": 10,\n",
    "          \"tree_method\":\"gpu_hist\",\n",
    "          \"gpu_id\":0\n",
    "          }\n",
    "num_boost_round = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGboost训练集与验证集\n",
    "dtrain = xgb.DMatrix(tai_xtrain, tai_ytrain)\n",
    "dvalid = xgb.DMatrix(tai_xtest, tai_ytest)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到序列\n",
    "from itertools import combinations\n",
    "\n",
    "def grid_dict(param_test):\n",
    "    combine_list = []\n",
    "    key_nums = 0\n",
    "    for i in param_test:\n",
    "        key_nums += 1\n",
    "        for j in param_test[i]:\n",
    "            combine_list.append({i:j})\n",
    "  \n",
    "    result = []\n",
    "    for i in combinations(combine_list, key_nums):\n",
    "        tmp_dict = {}\n",
    "        for j in list(i):\n",
    "            for key in j:\n",
    "                tmp_dict[key] = j[key]\n",
    "        if len(tmp_dict) == key_nums:\n",
    "            result.append(tmp_dict)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def train_model(params,flag = False):\n",
    "    print(\"Start Train...\")\n",
    "    start = time()\n",
    "    gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist,early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=flag)\n",
    "    end = time()\n",
    "    print('time is {:2f} s.'.format(end-start))\n",
    "    \n",
    "    print(\"testing...\")\n",
    "    tai_xtest.sort_index(inplace=True) \n",
    "    tai_ytest.sort_index(inplace=True) \n",
    "    yhat = gbm.predict(xgb.DMatrix(tai_xtest))\n",
    "    error = rmspe(np.expm1(tai_ytest), np.expm1(yhat))\n",
    "\n",
    "    print('RMSPE: {:.6f}'.format(error))\n",
    "    return error,gbm\n",
    "\n",
    "def grid_search_xg(params, params_grid):\n",
    "    iter_num = 1\n",
    "    grid_list = grid_dict(params_grid)\n",
    "    gbm = None\n",
    "    print('Iter nums is {:1f}.'.format(len(grid_list)))\n",
    "    min_error = 100\n",
    "    result_params = {}\n",
    "    for i in grid_list:\n",
    "        print('Iter No is {:1f}.'.format(iter_num))\n",
    "        for key in i:\n",
    "            params[key] = i[key]\n",
    "        print(params)\n",
    "        model_result = train_model(params)\n",
    "        error = model_result[0]\n",
    "        if error<min_error:\n",
    "            result_params = i\n",
    "            min_error = error\n",
    "            gbm = model_result[1]\n",
    "        iter_num += 1\n",
    "    \n",
    "    print('best_param:')\n",
    "    print(result_params)\n",
    "    \n",
    "    for key in result_params:\n",
    "        params[key] = result_params[key]\n",
    "    return gbm     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Train...\n",
      "[0]\ttrain-rmse:7.44334\teval-rmse:7.44788\ttrain-rmspe:0.999525\teval-rmspe:0.999529\n",
      "Multiple eval metrics have been passed: 'eval-rmspe' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmspe hasn't improved in 100 rounds.\n",
      "[1]\ttrain-rmse:6.70066\teval-rmse:6.70606\ttrain-rmspe:0.99882\teval-rmspe:0.998832\n",
      "[2]\ttrain-rmse:6.03251\teval-rmse:6.03779\ttrain-rmspe:0.997544\teval-rmspe:0.997569\n",
      "[3]\ttrain-rmse:5.4311\teval-rmse:5.43759\ttrain-rmspe:0.995402\teval-rmspe:0.995452\n",
      "[4]\ttrain-rmse:4.88999\teval-rmse:4.89789\ttrain-rmspe:0.992011\teval-rmspe:0.99211\n",
      "[5]\ttrain-rmse:4.4032\teval-rmse:4.41172\ttrain-rmspe:0.986936\teval-rmspe:0.987105\n",
      "[6]\ttrain-rmse:3.96519\teval-rmse:3.97338\ttrain-rmspe:0.979725\teval-rmspe:0.979978\n",
      "[7]\ttrain-rmse:3.57122\teval-rmse:3.57979\ttrain-rmspe:0.969934\teval-rmspe:0.970314\n",
      "[8]\ttrain-rmse:3.21687\teval-rmse:3.22455\ttrain-rmspe:0.957165\teval-rmspe:0.957653\n",
      "[9]\ttrain-rmse:2.8982\teval-rmse:2.90641\ttrain-rmspe:0.941148\teval-rmspe:0.941834\n",
      "[10]\ttrain-rmse:2.61151\teval-rmse:2.61727\ttrain-rmspe:0.921739\teval-rmspe:0.922426\n",
      "[11]\ttrain-rmse:2.35378\teval-rmse:2.35824\ttrain-rmspe:0.898911\teval-rmspe:0.899604\n",
      "[12]\ttrain-rmse:2.12196\teval-rmse:2.12604\ttrain-rmspe:0.872813\teval-rmspe:0.87356\n",
      "[13]\ttrain-rmse:1.91391\teval-rmse:1.91806\ttrain-rmspe:0.843621\teval-rmspe:0.844461\n",
      "[14]\ttrain-rmse:1.727\teval-rmse:1.72891\ttrain-rmspe:0.811769\teval-rmspe:0.812123\n",
      "[15]\ttrain-rmse:1.55936\teval-rmse:1.55692\ttrain-rmspe:0.777711\teval-rmspe:0.77691\n",
      "[16]\ttrain-rmse:1.40817\teval-rmse:1.40507\ttrain-rmspe:0.742201\teval-rmspe:0.740883\n",
      "[17]\ttrain-rmse:1.273\teval-rmse:1.26808\ttrain-rmspe:0.705521\teval-rmspe:0.703189\n",
      "[18]\ttrain-rmse:1.15162\teval-rmse:1.14666\ttrain-rmspe:0.668439\teval-rmspe:0.665425\n",
      "[19]\ttrain-rmse:1.0432\teval-rmse:1.03334\ttrain-rmspe:0.631318\teval-rmspe:0.625504\n",
      "[20]\ttrain-rmse:0.944473\teval-rmse:0.934591\ttrain-rmspe:0.594877\teval-rmspe:0.587938\n",
      "[21]\ttrain-rmse:0.856837\teval-rmse:0.847526\ttrain-rmspe:0.559325\teval-rmspe:0.551452\n",
      "[22]\ttrain-rmse:0.779399\teval-rmse:0.767617\ttrain-rmspe:0.525219\teval-rmspe:0.514685\n",
      "[23]\ttrain-rmse:0.709202\teval-rmse:0.697595\ttrain-rmspe:0.492755\teval-rmspe:0.480831\n",
      "[24]\ttrain-rmse:0.647696\teval-rmse:0.636556\ttrain-rmspe:0.462466\teval-rmspe:0.449059\n",
      "[25]\ttrain-rmse:0.591282\teval-rmse:0.580375\ttrain-rmspe:0.434075\teval-rmspe:0.41903\n",
      "[26]\ttrain-rmse:0.542952\teval-rmse:0.532324\ttrain-rmspe:0.408491\teval-rmspe:0.391826\n",
      "[27]\ttrain-rmse:0.499266\teval-rmse:0.489179\ttrain-rmspe:0.385062\teval-rmspe:0.36692\n",
      "[28]\ttrain-rmse:0.460757\teval-rmse:0.451386\ttrain-rmspe:0.364189\teval-rmspe:0.344664\n",
      "[29]\ttrain-rmse:0.427793\teval-rmse:0.416369\ttrain-rmspe:0.34627\teval-rmspe:0.323564\n",
      "[30]\ttrain-rmse:0.398774\teval-rmse:0.388289\ttrain-rmspe:0.330543\teval-rmspe:0.306807\n",
      "[31]\ttrain-rmse:0.372535\teval-rmse:0.362783\ttrain-rmspe:0.316521\teval-rmspe:0.291563\n",
      "[32]\ttrain-rmse:0.350335\teval-rmse:0.341113\ttrain-rmspe:0.30508\teval-rmspe:0.279034\n",
      "[33]\ttrain-rmse:0.330795\teval-rmse:0.322417\ttrain-rmspe:0.295199\teval-rmspe:0.268503\n",
      "[34]\ttrain-rmse:0.312882\teval-rmse:0.305049\ttrain-rmspe:0.286522\teval-rmspe:0.258786\n",
      "[35]\ttrain-rmse:0.29779\teval-rmse:0.29086\ttrain-rmspe:0.279744\teval-rmspe:0.251473\n",
      "[36]\ttrain-rmse:0.282646\teval-rmse:0.276823\ttrain-rmspe:0.272932\teval-rmspe:0.243526\n",
      "[37]\ttrain-rmse:0.26886\teval-rmse:0.26409\ttrain-rmspe:0.266305\teval-rmspe:0.236508\n",
      "[38]\ttrain-rmse:0.258162\teval-rmse:0.254417\ttrain-rmspe:0.261698\teval-rmspe:0.231797\n",
      "[39]\ttrain-rmse:0.247388\teval-rmse:0.245229\ttrain-rmspe:0.257179\teval-rmspe:0.227098\n",
      "[40]\ttrain-rmse:0.241043\teval-rmse:0.239781\ttrain-rmspe:0.255702\teval-rmspe:0.225972\n",
      "[41]\ttrain-rmse:0.235114\teval-rmse:0.234849\ttrain-rmspe:0.254071\teval-rmspe:0.224235\n",
      "[42]\ttrain-rmse:0.228608\teval-rmse:0.229307\ttrain-rmspe:0.251425\teval-rmspe:0.221713\n",
      "[43]\ttrain-rmse:0.224572\teval-rmse:0.226399\ttrain-rmspe:0.250939\teval-rmspe:0.221651\n",
      "[44]\ttrain-rmse:0.220605\teval-rmse:0.223223\ttrain-rmspe:0.250171\teval-rmspe:0.221223\n",
      "[45]\ttrain-rmse:0.216566\teval-rmse:0.21989\ttrain-rmspe:0.24851\teval-rmspe:0.220461\n",
      "[46]\ttrain-rmse:0.213456\teval-rmse:0.218132\ttrain-rmspe:0.247516\teval-rmspe:0.221041\n",
      "[47]\ttrain-rmse:0.2105\teval-rmse:0.216151\ttrain-rmspe:0.246687\teval-rmspe:0.220884\n",
      "[48]\ttrain-rmse:0.20743\teval-rmse:0.214342\ttrain-rmspe:0.245897\teval-rmspe:0.220887\n",
      "[49]\ttrain-rmse:0.205601\teval-rmse:0.213085\ttrain-rmspe:0.244245\teval-rmspe:0.221198\n",
      "[50]\ttrain-rmse:0.202542\teval-rmse:0.210758\ttrain-rmspe:0.242491\teval-rmspe:0.219961\n",
      "[51]\ttrain-rmse:0.201201\teval-rmse:0.210419\ttrain-rmspe:0.241666\teval-rmspe:0.221484\n",
      "[52]\ttrain-rmse:0.199351\teval-rmse:0.209188\ttrain-rmspe:0.235252\teval-rmspe:0.22143\n",
      "[53]\ttrain-rmse:0.194528\teval-rmse:0.205323\ttrain-rmspe:0.232382\teval-rmspe:0.218165\n",
      "[54]\ttrain-rmse:0.193364\teval-rmse:0.204971\ttrain-rmspe:0.232058\teval-rmspe:0.219452\n",
      "[55]\ttrain-rmse:0.191004\teval-rmse:0.203571\ttrain-rmspe:0.226809\teval-rmspe:0.218973\n",
      "[56]\ttrain-rmse:0.188705\teval-rmse:0.201608\ttrain-rmspe:0.22554\teval-rmspe:0.217431\n",
      "[57]\ttrain-rmse:0.186486\teval-rmse:0.200003\ttrain-rmspe:0.224109\teval-rmspe:0.216296\n",
      "[58]\ttrain-rmse:0.185615\teval-rmse:0.199834\ttrain-rmspe:0.223823\teval-rmspe:0.217079\n",
      "[59]\ttrain-rmse:0.18405\teval-rmse:0.198698\ttrain-rmspe:0.222878\teval-rmspe:0.216195\n",
      "[60]\ttrain-rmse:0.18262\teval-rmse:0.196479\ttrain-rmspe:0.222039\teval-rmspe:0.213926\n",
      "[61]\ttrain-rmse:0.179345\teval-rmse:0.1939\ttrain-rmspe:0.218317\teval-rmspe:0.211446\n",
      "[62]\ttrain-rmse:0.17763\teval-rmse:0.192499\ttrain-rmspe:0.213107\teval-rmspe:0.210019\n",
      "[63]\ttrain-rmse:0.176352\teval-rmse:0.191432\ttrain-rmspe:0.212267\teval-rmspe:0.20911\n",
      "[64]\ttrain-rmse:0.173961\teval-rmse:0.18942\ttrain-rmspe:0.210065\teval-rmspe:0.207231\n",
      "[65]\ttrain-rmse:0.173094\teval-rmse:0.18878\ttrain-rmspe:0.209605\teval-rmspe:0.206774\n",
      "[66]\ttrain-rmse:0.171818\teval-rmse:0.187696\ttrain-rmspe:0.208587\teval-rmspe:0.205762\n",
      "[67]\ttrain-rmse:0.170411\teval-rmse:0.1867\ttrain-rmspe:0.204876\teval-rmspe:0.204815\n",
      "[68]\ttrain-rmse:0.169423\teval-rmse:0.186137\ttrain-rmspe:0.203823\teval-rmspe:0.204483\n",
      "[69]\ttrain-rmse:0.167922\teval-rmse:0.184864\ttrain-rmspe:0.202532\teval-rmspe:0.203244\n",
      "[70]\ttrain-rmse:0.16757\teval-rmse:0.184686\ttrain-rmspe:0.202319\teval-rmspe:0.203159\n",
      "[71]\ttrain-rmse:0.166416\teval-rmse:0.183678\ttrain-rmspe:0.201304\teval-rmspe:0.202078\n",
      "[72]\ttrain-rmse:0.165189\teval-rmse:0.182667\ttrain-rmspe:0.200218\teval-rmspe:0.201017\n",
      "[73]\ttrain-rmse:0.163942\teval-rmse:0.181708\ttrain-rmspe:0.198708\teval-rmspe:0.200019\n",
      "[74]\ttrain-rmse:0.163488\teval-rmse:0.181287\ttrain-rmspe:0.198009\teval-rmspe:0.199652\n",
      "[75]\ttrain-rmse:0.160564\teval-rmse:0.178902\ttrain-rmspe:0.194112\teval-rmspe:0.196962\n",
      "[76]\ttrain-rmse:0.157855\teval-rmse:0.17693\ttrain-rmspe:0.191578\teval-rmspe:0.194894\n",
      "[77]\ttrain-rmse:0.155729\teval-rmse:0.175089\ttrain-rmspe:0.189476\teval-rmspe:0.192679\n",
      "[78]\ttrain-rmse:0.154184\teval-rmse:0.173876\ttrain-rmspe:0.188036\teval-rmspe:0.191417\n",
      "[79]\ttrain-rmse:0.153726\teval-rmse:0.173525\ttrain-rmspe:0.183904\teval-rmspe:0.191014\n",
      "[80]\ttrain-rmse:0.151522\teval-rmse:0.171615\ttrain-rmspe:0.181812\teval-rmspe:0.188816\n",
      "[81]\ttrain-rmse:0.14884\teval-rmse:0.169276\ttrain-rmspe:0.179385\teval-rmspe:0.186184\n",
      "[82]\ttrain-rmse:0.147781\teval-rmse:0.168378\ttrain-rmspe:0.178255\teval-rmspe:0.185312\n",
      "[83]\ttrain-rmse:0.146419\teval-rmse:0.167087\ttrain-rmspe:0.177051\teval-rmspe:0.18381\n",
      "[84]\ttrain-rmse:0.146037\teval-rmse:0.167053\ttrain-rmspe:0.176534\teval-rmspe:0.183802\n",
      "[85]\ttrain-rmse:0.145126\teval-rmse:0.16624\ttrain-rmspe:0.175684\teval-rmspe:0.182894\n",
      "[86]\ttrain-rmse:0.144502\teval-rmse:0.16486\ttrain-rmspe:0.173641\teval-rmspe:0.181445\n",
      "[87]\ttrain-rmse:0.14396\teval-rmse:0.164527\ttrain-rmspe:0.173063\teval-rmspe:0.181117\n",
      "[88]\ttrain-rmse:0.143127\teval-rmse:0.163861\ttrain-rmspe:0.172274\teval-rmspe:0.180391\n",
      "[89]\ttrain-rmse:0.142523\teval-rmse:0.163317\ttrain-rmspe:0.171738\teval-rmspe:0.179788\n",
      "[90]\ttrain-rmse:0.140838\teval-rmse:0.161919\ttrain-rmspe:0.170102\teval-rmspe:0.178154\n",
      "[91]\ttrain-rmse:0.139939\teval-rmse:0.161176\ttrain-rmspe:0.16929\teval-rmspe:0.177372\n",
      "[92]\ttrain-rmse:0.139629\teval-rmse:0.160887\ttrain-rmspe:0.1688\teval-rmspe:0.177006\n",
      "[93]\ttrain-rmse:0.139204\teval-rmse:0.160636\ttrain-rmspe:0.168333\teval-rmspe:0.176366\n",
      "[94]\ttrain-rmse:0.137367\teval-rmse:0.15918\ttrain-rmspe:0.166385\teval-rmspe:0.174734\n",
      "[95]\ttrain-rmse:0.137001\teval-rmse:0.158588\ttrain-rmspe:0.166037\teval-rmspe:0.173792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96]\ttrain-rmse:0.136019\teval-rmse:0.157701\ttrain-rmspe:0.164903\teval-rmspe:0.17275\n",
      "[97]\ttrain-rmse:0.135451\teval-rmse:0.157137\ttrain-rmspe:0.164374\teval-rmspe:0.172029\n",
      "[98]\ttrain-rmse:0.135092\teval-rmse:0.156858\ttrain-rmspe:0.164079\teval-rmspe:0.171693\n",
      "[99]\ttrain-rmse:0.134272\teval-rmse:0.15624\ttrain-rmspe:0.16326\teval-rmspe:0.170951\n",
      "[100]\ttrain-rmse:0.133787\teval-rmse:0.155877\ttrain-rmspe:0.162553\teval-rmspe:0.170417\n",
      "[101]\ttrain-rmse:0.133336\teval-rmse:0.155542\ttrain-rmspe:0.161185\teval-rmspe:0.17006\n",
      "[102]\ttrain-rmse:0.131522\teval-rmse:0.154082\ttrain-rmspe:0.158837\teval-rmspe:0.168379\n",
      "[103]\ttrain-rmse:0.130872\teval-rmse:0.153574\ttrain-rmspe:0.15822\teval-rmspe:0.167819\n",
      "[104]\ttrain-rmse:0.130631\teval-rmse:0.153481\ttrain-rmspe:0.157816\teval-rmspe:0.167656\n",
      "[105]\ttrain-rmse:0.130148\teval-rmse:0.153097\ttrain-rmspe:0.157288\teval-rmspe:0.167202\n",
      "[106]\ttrain-rmse:0.128565\teval-rmse:0.151844\ttrain-rmspe:0.155724\teval-rmspe:0.165769\n",
      "[107]\ttrain-rmse:0.128353\teval-rmse:0.151379\ttrain-rmspe:0.155449\teval-rmspe:0.165044\n",
      "[108]\ttrain-rmse:0.126945\teval-rmse:0.150273\ttrain-rmspe:0.15406\teval-rmspe:0.163807\n",
      "[109]\ttrain-rmse:0.126347\teval-rmse:0.149772\ttrain-rmspe:0.153495\teval-rmspe:0.163225\n",
      "[110]\ttrain-rmse:0.125704\teval-rmse:0.149289\ttrain-rmspe:0.152528\teval-rmspe:0.162662\n",
      "[111]\ttrain-rmse:0.125199\teval-rmse:0.148946\ttrain-rmspe:0.152007\teval-rmspe:0.162262\n",
      "[112]\ttrain-rmse:0.124894\teval-rmse:0.148506\ttrain-rmspe:0.15158\teval-rmspe:0.161712\n",
      "[113]\ttrain-rmse:0.123896\teval-rmse:0.147729\ttrain-rmspe:0.150631\teval-rmspe:0.16084\n",
      "[114]\ttrain-rmse:0.123386\teval-rmse:0.147397\ttrain-rmspe:0.149995\teval-rmspe:0.160475\n",
      "[115]\ttrain-rmse:0.122895\teval-rmse:0.146857\ttrain-rmspe:0.149455\teval-rmspe:0.15969\n",
      "[116]\ttrain-rmse:0.12243\teval-rmse:0.146589\ttrain-rmspe:0.149011\teval-rmspe:0.159359\n",
      "[117]\ttrain-rmse:0.121528\teval-rmse:0.145904\ttrain-rmspe:0.148166\teval-rmspe:0.158542\n",
      "[118]\ttrain-rmse:0.120692\teval-rmse:0.145248\ttrain-rmspe:0.147299\teval-rmspe:0.157692\n",
      "[119]\ttrain-rmse:0.120366\teval-rmse:0.145055\ttrain-rmspe:0.146979\teval-rmspe:0.157483\n",
      "[120]\ttrain-rmse:0.120054\teval-rmse:0.14477\ttrain-rmspe:0.146634\teval-rmspe:0.157123\n",
      "[121]\ttrain-rmse:0.119776\teval-rmse:0.144598\ttrain-rmspe:0.146415\teval-rmspe:0.15692\n",
      "[122]\ttrain-rmse:0.119567\teval-rmse:0.144472\ttrain-rmspe:0.145648\teval-rmspe:0.156755\n",
      "[123]\ttrain-rmse:0.11931\teval-rmse:0.144376\ttrain-rmspe:0.145326\teval-rmspe:0.156646\n",
      "[124]\ttrain-rmse:0.118841\teval-rmse:0.144019\ttrain-rmspe:0.144884\teval-rmspe:0.156242\n",
      "[125]\ttrain-rmse:0.118502\teval-rmse:0.143848\ttrain-rmspe:0.143818\teval-rmspe:0.156047\n",
      "[126]\ttrain-rmse:0.118077\teval-rmse:0.143529\ttrain-rmspe:0.143402\teval-rmspe:0.155687\n",
      "[127]\ttrain-rmse:0.117748\teval-rmse:0.143239\ttrain-rmspe:0.142545\teval-rmspe:0.155379\n",
      "[128]\ttrain-rmse:0.117072\teval-rmse:0.1427\ttrain-rmspe:0.141828\teval-rmspe:0.154762\n",
      "[129]\ttrain-rmse:0.116216\teval-rmse:0.142025\ttrain-rmspe:0.139096\teval-rmspe:0.153976\n",
      "[130]\ttrain-rmse:0.115909\teval-rmse:0.141783\ttrain-rmspe:0.138756\teval-rmspe:0.153676\n",
      "[131]\ttrain-rmse:0.115627\teval-rmse:0.141551\ttrain-rmspe:0.138508\teval-rmspe:0.153399\n",
      "[132]\ttrain-rmse:0.115179\teval-rmse:0.141375\ttrain-rmspe:0.138061\teval-rmspe:0.153199\n",
      "[133]\ttrain-rmse:0.114966\teval-rmse:0.141228\ttrain-rmspe:0.137801\teval-rmspe:0.153022\n",
      "[134]\ttrain-rmse:0.114472\teval-rmse:0.140835\ttrain-rmspe:0.137304\teval-rmspe:0.152539\n",
      "[135]\ttrain-rmse:0.11419\teval-rmse:0.140712\ttrain-rmspe:0.137042\teval-rmspe:0.152417\n",
      "[136]\ttrain-rmse:0.113867\teval-rmse:0.140666\ttrain-rmspe:0.136693\teval-rmspe:0.15227\n",
      "[137]\ttrain-rmse:0.113666\teval-rmse:0.14037\ttrain-rmspe:0.136486\teval-rmspe:0.151863\n",
      "[138]\ttrain-rmse:0.113393\teval-rmse:0.140188\ttrain-rmspe:0.136227\teval-rmspe:0.151684\n",
      "[139]\ttrain-rmse:0.112839\teval-rmse:0.139951\ttrain-rmspe:0.135362\teval-rmspe:0.151428\n",
      "[140]\ttrain-rmse:0.11258\teval-rmse:0.139759\ttrain-rmspe:0.134753\teval-rmspe:0.151235\n",
      "[141]\ttrain-rmse:0.112343\teval-rmse:0.139661\ttrain-rmspe:0.134523\teval-rmspe:0.151135\n",
      "[142]\ttrain-rmse:0.112193\teval-rmse:0.13961\ttrain-rmspe:0.134344\teval-rmspe:0.151065\n",
      "[143]\ttrain-rmse:0.111996\teval-rmse:0.139357\ttrain-rmspe:0.13417\teval-rmspe:0.150704\n",
      "[144]\ttrain-rmse:0.111772\teval-rmse:0.139195\ttrain-rmspe:0.13393\teval-rmspe:0.150528\n",
      "[145]\ttrain-rmse:0.111301\teval-rmse:0.138804\ttrain-rmspe:0.133459\teval-rmspe:0.15005\n",
      "[146]\ttrain-rmse:0.110777\teval-rmse:0.138402\ttrain-rmspe:0.132964\teval-rmspe:0.149592\n",
      "[147]\ttrain-rmse:0.110495\teval-rmse:0.138184\ttrain-rmspe:0.132615\teval-rmspe:0.149344\n",
      "[148]\ttrain-rmse:0.110014\teval-rmse:0.137777\ttrain-rmspe:0.132153\teval-rmspe:0.148899\n",
      "[149]\ttrain-rmse:0.109691\teval-rmse:0.137595\ttrain-rmspe:0.131828\teval-rmspe:0.148697\n",
      "[150]\ttrain-rmse:0.109297\teval-rmse:0.13728\ttrain-rmspe:0.131441\teval-rmspe:0.148351\n",
      "[151]\ttrain-rmse:0.109089\teval-rmse:0.136964\ttrain-rmspe:0.130972\teval-rmspe:0.147892\n",
      "[152]\ttrain-rmse:0.108822\teval-rmse:0.13681\ttrain-rmspe:0.130667\teval-rmspe:0.147734\n",
      "[153]\ttrain-rmse:0.108679\teval-rmse:0.136671\ttrain-rmspe:0.130473\teval-rmspe:0.14758\n",
      "[154]\ttrain-rmse:0.108371\teval-rmse:0.136607\ttrain-rmspe:0.12828\teval-rmspe:0.147543\n",
      "[155]\ttrain-rmse:0.108214\teval-rmse:0.136588\ttrain-rmspe:0.12812\teval-rmspe:0.147583\n",
      "[156]\ttrain-rmse:0.10793\teval-rmse:0.136374\ttrain-rmspe:0.127757\teval-rmspe:0.147339\n",
      "[157]\ttrain-rmse:0.107705\teval-rmse:0.136214\ttrain-rmspe:0.12752\teval-rmspe:0.147185\n",
      "[158]\ttrain-rmse:0.107239\teval-rmse:0.13587\ttrain-rmspe:0.126973\teval-rmspe:0.146761\n",
      "[159]\ttrain-rmse:0.107114\teval-rmse:0.136018\ttrain-rmspe:0.126806\teval-rmspe:0.147001\n",
      "[160]\ttrain-rmse:0.106957\teval-rmse:0.1359\ttrain-rmspe:0.12667\teval-rmspe:0.146877\n",
      "[161]\ttrain-rmse:0.1068\teval-rmse:0.135776\ttrain-rmspe:0.126515\teval-rmspe:0.146748\n",
      "[162]\ttrain-rmse:0.106608\teval-rmse:0.135639\ttrain-rmspe:0.126312\teval-rmspe:0.14661\n",
      "[163]\ttrain-rmse:0.10638\teval-rmse:0.135479\ttrain-rmspe:0.126113\teval-rmspe:0.146424\n",
      "[164]\ttrain-rmse:0.106263\teval-rmse:0.135521\ttrain-rmspe:0.12597\teval-rmspe:0.146523\n",
      "[165]\ttrain-rmse:0.105866\teval-rmse:0.135218\ttrain-rmspe:0.125585\teval-rmspe:0.14615\n",
      "[166]\ttrain-rmse:0.105633\teval-rmse:0.135023\ttrain-rmspe:0.125343\teval-rmspe:0.145964\n",
      "[167]\ttrain-rmse:0.10557\teval-rmse:0.134748\ttrain-rmspe:0.125279\teval-rmspe:0.145592\n",
      "[168]\ttrain-rmse:0.105486\teval-rmse:0.134577\ttrain-rmspe:0.125197\teval-rmspe:0.145385\n",
      "[169]\ttrain-rmse:0.105372\teval-rmse:0.134499\ttrain-rmspe:0.125094\teval-rmspe:0.145311\n",
      "[170]\ttrain-rmse:0.104884\teval-rmse:0.134184\ttrain-rmspe:0.124623\teval-rmspe:0.144968\n",
      "[171]\ttrain-rmse:0.104731\teval-rmse:0.134108\ttrain-rmspe:0.124474\teval-rmspe:0.144877\n",
      "[172]\ttrain-rmse:0.104499\teval-rmse:0.133888\ttrain-rmspe:0.124236\teval-rmspe:0.144645\n",
      "[173]\ttrain-rmse:0.104343\teval-rmse:0.133691\ttrain-rmspe:0.124041\teval-rmspe:0.144404\n",
      "[174]\ttrain-rmse:0.104177\teval-rmse:0.133593\ttrain-rmspe:0.12385\teval-rmspe:0.144294\n",
      "[175]\ttrain-rmse:0.104062\teval-rmse:0.133527\ttrain-rmspe:0.123693\teval-rmspe:0.144234\n",
      "[176]\ttrain-rmse:0.10394\teval-rmse:0.133509\ttrain-rmspe:0.123522\teval-rmspe:0.144194\n",
      "[177]\ttrain-rmse:0.103495\teval-rmse:0.133252\ttrain-rmspe:0.123127\teval-rmspe:0.143878\n",
      "[178]\ttrain-rmse:0.103406\teval-rmse:0.133252\ttrain-rmspe:0.123024\teval-rmspe:0.143886\n",
      "[179]\ttrain-rmse:0.103336\teval-rmse:0.133217\ttrain-rmspe:0.122959\teval-rmspe:0.14385\n",
      "[180]\ttrain-rmse:0.103195\teval-rmse:0.133126\ttrain-rmspe:0.122809\teval-rmspe:0.143754\n",
      "[181]\ttrain-rmse:0.102938\teval-rmse:0.132938\ttrain-rmspe:0.122564\teval-rmspe:0.143553\n",
      "[182]\ttrain-rmse:0.102777\teval-rmse:0.132842\ttrain-rmspe:0.122407\teval-rmspe:0.143459\n",
      "[183]\ttrain-rmse:0.102398\teval-rmse:0.132621\ttrain-rmspe:0.122037\teval-rmspe:0.143219\n",
      "[184]\ttrain-rmse:0.102291\teval-rmse:0.13256\ttrain-rmspe:0.121823\teval-rmspe:0.143153\n",
      "[185]\ttrain-rmse:0.102182\teval-rmse:0.132496\ttrain-rmspe:0.120939\teval-rmspe:0.143033\n",
      "[186]\ttrain-rmse:0.101965\teval-rmse:0.132343\ttrain-rmspe:0.120728\teval-rmspe:0.14286\n",
      "[187]\ttrain-rmse:0.101801\teval-rmse:0.132314\ttrain-rmspe:0.120495\teval-rmspe:0.14284\n",
      "[188]\ttrain-rmse:0.101584\teval-rmse:0.13219\ttrain-rmspe:0.120271\teval-rmspe:0.14272\n",
      "[189]\ttrain-rmse:0.101423\teval-rmse:0.132054\ttrain-rmspe:0.120116\teval-rmspe:0.142568\n",
      "[190]\ttrain-rmse:0.101249\teval-rmse:0.132022\ttrain-rmspe:0.119958\teval-rmspe:0.142552\n",
      "[191]\ttrain-rmse:0.101165\teval-rmse:0.132012\ttrain-rmspe:0.119873\teval-rmspe:0.142553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[192]\ttrain-rmse:0.101043\teval-rmse:0.13193\ttrain-rmspe:0.119709\teval-rmspe:0.14249\n",
      "[193]\ttrain-rmse:0.100849\teval-rmse:0.131782\ttrain-rmspe:0.119522\teval-rmspe:0.142305\n",
      "[194]\ttrain-rmse:0.100777\teval-rmse:0.131654\ttrain-rmspe:0.119404\teval-rmspe:0.142135\n",
      "[195]\ttrain-rmse:0.100525\teval-rmse:0.131507\ttrain-rmspe:0.118336\teval-rmspe:0.14197\n",
      "[196]\ttrain-rmse:0.100365\teval-rmse:0.131386\ttrain-rmspe:0.118179\teval-rmspe:0.141833\n",
      "[197]\ttrain-rmse:0.100184\teval-rmse:0.131413\ttrain-rmspe:0.117944\teval-rmspe:0.141896\n",
      "[198]\ttrain-rmse:0.100076\teval-rmse:0.131352\ttrain-rmspe:0.117788\teval-rmspe:0.141819\n",
      "[199]\ttrain-rmse:0.099709\teval-rmse:0.131074\ttrain-rmspe:0.117425\teval-rmspe:0.141515\n",
      "[200]\ttrain-rmse:0.099531\teval-rmse:0.130928\ttrain-rmspe:0.117195\teval-rmspe:0.141349\n",
      "[201]\ttrain-rmse:0.099369\teval-rmse:0.130839\ttrain-rmspe:0.116992\teval-rmspe:0.141257\n",
      "[202]\ttrain-rmse:0.099247\teval-rmse:0.130804\ttrain-rmspe:0.116869\teval-rmspe:0.141221\n",
      "[203]\ttrain-rmse:0.099099\teval-rmse:0.130687\ttrain-rmspe:0.116707\teval-rmspe:0.141128\n",
      "[204]\ttrain-rmse:0.098988\teval-rmse:0.130637\ttrain-rmspe:0.116597\teval-rmspe:0.141073\n",
      "[205]\ttrain-rmse:0.098909\teval-rmse:0.130619\ttrain-rmspe:0.116525\teval-rmspe:0.141051\n",
      "[206]\ttrain-rmse:0.098637\teval-rmse:0.130432\ttrain-rmspe:0.116231\teval-rmspe:0.14083\n",
      "[207]\ttrain-rmse:0.098312\teval-rmse:0.130219\ttrain-rmspe:0.115915\teval-rmspe:0.140585\n",
      "[208]\ttrain-rmse:0.098199\teval-rmse:0.130152\ttrain-rmspe:0.115808\teval-rmspe:0.140547\n",
      "[209]\ttrain-rmse:0.098012\teval-rmse:0.130056\ttrain-rmspe:0.115621\teval-rmspe:0.140438\n",
      "[210]\ttrain-rmse:0.097882\teval-rmse:0.13008\ttrain-rmspe:0.115443\teval-rmspe:0.140477\n",
      "[211]\ttrain-rmse:0.097789\teval-rmse:0.130094\ttrain-rmspe:0.115355\teval-rmspe:0.140514\n",
      "[212]\ttrain-rmse:0.097663\teval-rmse:0.129971\ttrain-rmspe:0.114981\teval-rmspe:0.140388\n",
      "[213]\ttrain-rmse:0.097541\teval-rmse:0.1299\ttrain-rmspe:0.114753\teval-rmspe:0.140312\n",
      "[214]\ttrain-rmse:0.09727\teval-rmse:0.129719\ttrain-rmspe:0.114503\teval-rmspe:0.140095\n",
      "[215]\ttrain-rmse:0.097186\teval-rmse:0.129669\ttrain-rmspe:0.114391\teval-rmspe:0.140042\n",
      "[216]\ttrain-rmse:0.097037\teval-rmse:0.129581\ttrain-rmspe:0.114177\teval-rmspe:0.139936\n",
      "[217]\ttrain-rmse:0.096859\teval-rmse:0.129463\ttrain-rmspe:0.113999\teval-rmspe:0.139802\n",
      "[218]\ttrain-rmse:0.096743\teval-rmse:0.129422\ttrain-rmspe:0.113883\teval-rmspe:0.139766\n",
      "[219]\ttrain-rmse:0.096602\teval-rmse:0.129334\ttrain-rmspe:0.113739\teval-rmspe:0.139661\n",
      "[220]\ttrain-rmse:0.096458\teval-rmse:0.12923\ttrain-rmspe:0.113592\teval-rmspe:0.13955\n",
      "[221]\ttrain-rmse:0.096378\teval-rmse:0.129548\ttrain-rmspe:0.113423\teval-rmspe:0.14001\n",
      "[222]\ttrain-rmse:0.096322\teval-rmse:0.129577\ttrain-rmspe:0.113347\teval-rmspe:0.140045\n",
      "[223]\ttrain-rmse:0.096176\teval-rmse:0.129467\ttrain-rmspe:0.113152\teval-rmspe:0.139922\n",
      "[224]\ttrain-rmse:0.096114\teval-rmse:0.129523\ttrain-rmspe:0.113082\teval-rmspe:0.139991\n",
      "[225]\ttrain-rmse:0.096035\teval-rmse:0.129494\ttrain-rmspe:0.112209\teval-rmspe:0.13996\n",
      "[226]\ttrain-rmse:0.095979\teval-rmse:0.129454\ttrain-rmspe:0.111606\teval-rmspe:0.13992\n",
      "[227]\ttrain-rmse:0.095872\teval-rmse:0.129394\ttrain-rmspe:0.111496\teval-rmspe:0.139857\n",
      "[228]\ttrain-rmse:0.095717\teval-rmse:0.12926\ttrain-rmspe:0.111328\teval-rmspe:0.139734\n",
      "[229]\ttrain-rmse:0.095491\teval-rmse:0.129136\ttrain-rmspe:0.110998\teval-rmspe:0.13954\n",
      "[230]\ttrain-rmse:0.095313\teval-rmse:0.12909\ttrain-rmspe:0.110809\teval-rmspe:0.139482\n",
      "[231]\ttrain-rmse:0.095206\teval-rmse:0.129037\ttrain-rmspe:0.110648\teval-rmspe:0.139429\n",
      "[232]\ttrain-rmse:0.095117\teval-rmse:0.12875\ttrain-rmspe:0.110131\teval-rmspe:0.138967\n",
      "[233]\ttrain-rmse:0.095039\teval-rmse:0.128727\ttrain-rmspe:0.10994\teval-rmspe:0.138948\n",
      "[234]\ttrain-rmse:0.094942\teval-rmse:0.128696\ttrain-rmspe:0.109794\teval-rmspe:0.138909\n",
      "[235]\ttrain-rmse:0.094857\teval-rmse:0.128656\ttrain-rmspe:0.109715\teval-rmspe:0.138872\n",
      "[236]\ttrain-rmse:0.094734\teval-rmse:0.128591\ttrain-rmspe:0.109601\teval-rmspe:0.138765\n",
      "[237]\ttrain-rmse:0.094437\teval-rmse:0.128421\ttrain-rmspe:0.109298\teval-rmspe:0.138585\n",
      "[238]\ttrain-rmse:0.094314\teval-rmse:0.128313\ttrain-rmspe:0.109008\teval-rmspe:0.13846\n",
      "[239]\ttrain-rmse:0.094213\teval-rmse:0.128304\ttrain-rmspe:0.108892\teval-rmspe:0.138458\n",
      "[240]\ttrain-rmse:0.094149\teval-rmse:0.128298\ttrain-rmspe:0.10883\teval-rmspe:0.138451\n",
      "[241]\ttrain-rmse:0.094004\teval-rmse:0.128232\ttrain-rmspe:0.108688\teval-rmspe:0.138377\n",
      "[242]\ttrain-rmse:0.093925\teval-rmse:0.128128\ttrain-rmspe:0.108531\teval-rmspe:0.138242\n",
      "[243]\ttrain-rmse:0.093867\teval-rmse:0.128031\ttrain-rmspe:0.108492\teval-rmspe:0.138128\n",
      "[244]\ttrain-rmse:0.093739\teval-rmse:0.127951\ttrain-rmspe:0.108081\teval-rmspe:0.138044\n",
      "[245]\ttrain-rmse:0.093634\teval-rmse:0.127897\ttrain-rmspe:0.107975\teval-rmspe:0.137978\n",
      "[246]\ttrain-rmse:0.093542\teval-rmse:0.127821\ttrain-rmspe:0.107889\teval-rmspe:0.137898\n",
      "[247]\ttrain-rmse:0.093309\teval-rmse:0.127707\ttrain-rmspe:0.107658\teval-rmspe:0.137788\n",
      "[248]\ttrain-rmse:0.093133\teval-rmse:0.127606\ttrain-rmspe:0.107486\teval-rmspe:0.137679\n",
      "[249]\ttrain-rmse:0.09307\teval-rmse:0.127626\ttrain-rmspe:0.107423\teval-rmspe:0.137708\n",
      "[250]\ttrain-rmse:0.093016\teval-rmse:0.127686\ttrain-rmspe:0.107368\teval-rmspe:0.137784\n",
      "[251]\ttrain-rmse:0.092924\teval-rmse:0.127638\ttrain-rmspe:0.107261\teval-rmspe:0.137735\n",
      "[252]\ttrain-rmse:0.09281\teval-rmse:0.12756\ttrain-rmspe:0.107026\teval-rmspe:0.137643\n",
      "[253]\ttrain-rmse:0.092701\teval-rmse:0.12747\ttrain-rmspe:0.106902\teval-rmspe:0.137542\n",
      "[254]\ttrain-rmse:0.092626\teval-rmse:0.127483\ttrain-rmspe:0.10682\teval-rmspe:0.137574\n",
      "[255]\ttrain-rmse:0.09248\teval-rmse:0.12736\ttrain-rmspe:0.10666\teval-rmspe:0.137431\n",
      "[256]\ttrain-rmse:0.092386\teval-rmse:0.127348\ttrain-rmspe:0.106556\teval-rmspe:0.137412\n",
      "[257]\ttrain-rmse:0.092338\teval-rmse:0.127275\ttrain-rmspe:0.106513\teval-rmspe:0.137332\n",
      "[258]\ttrain-rmse:0.092194\teval-rmse:0.127231\ttrain-rmspe:0.106364\teval-rmspe:0.1373\n",
      "[259]\ttrain-rmse:0.092104\teval-rmse:0.12721\ttrain-rmspe:0.106216\teval-rmspe:0.137293\n",
      "[260]\ttrain-rmse:0.092036\teval-rmse:0.127192\ttrain-rmspe:0.106149\teval-rmspe:0.137287\n",
      "[261]\ttrain-rmse:0.091957\teval-rmse:0.127147\ttrain-rmspe:0.106053\teval-rmspe:0.137242\n",
      "[262]\ttrain-rmse:0.091904\teval-rmse:0.127105\ttrain-rmspe:0.106002\teval-rmspe:0.137194\n",
      "[263]\ttrain-rmse:0.091824\teval-rmse:0.127068\ttrain-rmspe:0.105805\teval-rmspe:0.13715\n",
      "[264]\ttrain-rmse:0.091712\teval-rmse:0.127022\ttrain-rmspe:0.105694\teval-rmspe:0.137093\n",
      "[265]\ttrain-rmse:0.09166\teval-rmse:0.127019\ttrain-rmspe:0.105642\teval-rmspe:0.137089\n",
      "[266]\ttrain-rmse:0.09155\teval-rmse:0.126971\ttrain-rmspe:0.105535\teval-rmspe:0.137004\n",
      "[267]\ttrain-rmse:0.091429\teval-rmse:0.126838\ttrain-rmspe:0.105256\teval-rmspe:0.136825\n",
      "[268]\ttrain-rmse:0.091309\teval-rmse:0.126867\ttrain-rmspe:0.105109\teval-rmspe:0.136835\n",
      "[269]\ttrain-rmse:0.091229\teval-rmse:0.126855\ttrain-rmspe:0.10502\teval-rmspe:0.136825\n",
      "[270]\ttrain-rmse:0.091175\teval-rmse:0.126832\ttrain-rmspe:0.104952\teval-rmspe:0.136807\n",
      "[271]\ttrain-rmse:0.091032\teval-rmse:0.126749\ttrain-rmspe:0.104769\teval-rmspe:0.136703\n",
      "[272]\ttrain-rmse:0.090957\teval-rmse:0.126831\ttrain-rmspe:0.104689\teval-rmspe:0.136763\n",
      "[273]\ttrain-rmse:0.090834\teval-rmse:0.126753\ttrain-rmspe:0.104568\teval-rmspe:0.136681\n",
      "[274]\ttrain-rmse:0.090746\teval-rmse:0.126761\ttrain-rmspe:0.104463\teval-rmspe:0.136694\n",
      "[275]\ttrain-rmse:0.090653\teval-rmse:0.126762\ttrain-rmspe:0.104284\teval-rmspe:0.136689\n",
      "[276]\ttrain-rmse:0.090582\teval-rmse:0.126706\ttrain-rmspe:0.104196\teval-rmspe:0.136605\n",
      "[277]\ttrain-rmse:0.090523\teval-rmse:0.126822\ttrain-rmspe:0.104138\teval-rmspe:0.136752\n",
      "[278]\ttrain-rmse:0.09043\teval-rmse:0.126764\ttrain-rmspe:0.104027\teval-rmspe:0.136679\n",
      "[279]\ttrain-rmse:0.090355\teval-rmse:0.126738\ttrain-rmspe:0.103944\teval-rmspe:0.136637\n",
      "[280]\ttrain-rmse:0.090268\teval-rmse:0.126716\ttrain-rmspe:0.1038\teval-rmspe:0.136627\n",
      "[281]\ttrain-rmse:0.090188\teval-rmse:0.126832\ttrain-rmspe:0.103678\teval-rmspe:0.136745\n",
      "[282]\ttrain-rmse:0.090122\teval-rmse:0.12679\ttrain-rmspe:0.103616\teval-rmspe:0.136709\n",
      "[283]\ttrain-rmse:0.090041\teval-rmse:0.126769\ttrain-rmspe:0.103514\teval-rmspe:0.136688\n",
      "[284]\ttrain-rmse:0.089972\teval-rmse:0.126721\ttrain-rmspe:0.103448\teval-rmspe:0.136625\n",
      "[285]\ttrain-rmse:0.089865\teval-rmse:0.126701\ttrain-rmspe:0.103264\teval-rmspe:0.136608\n",
      "[286]\ttrain-rmse:0.089802\teval-rmse:0.126719\ttrain-rmspe:0.103111\teval-rmspe:0.136637\n",
      "[287]\ttrain-rmse:0.089717\teval-rmse:0.126715\ttrain-rmspe:0.102983\teval-rmspe:0.136638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[288]\ttrain-rmse:0.089654\teval-rmse:0.126685\ttrain-rmspe:0.102911\teval-rmspe:0.136595\n",
      "[289]\ttrain-rmse:0.089586\teval-rmse:0.126672\ttrain-rmspe:0.102751\teval-rmspe:0.136581\n",
      "[290]\ttrain-rmse:0.089478\teval-rmse:0.12665\ttrain-rmspe:0.102608\teval-rmspe:0.136541\n",
      "[291]\ttrain-rmse:0.089426\teval-rmse:0.126704\ttrain-rmspe:0.102558\teval-rmspe:0.136603\n",
      "[292]\ttrain-rmse:0.089361\teval-rmse:0.126685\ttrain-rmspe:0.102464\teval-rmspe:0.136569\n",
      "[293]\ttrain-rmse:0.089208\teval-rmse:0.12659\ttrain-rmspe:0.102311\teval-rmspe:0.136466\n",
      "[294]\ttrain-rmse:0.089107\teval-rmse:0.126529\ttrain-rmspe:0.102207\teval-rmspe:0.136393\n",
      "[295]\ttrain-rmse:0.089033\teval-rmse:0.126539\ttrain-rmspe:0.102131\teval-rmspe:0.136416\n",
      "[296]\ttrain-rmse:0.088907\teval-rmse:0.126481\ttrain-rmspe:0.101998\teval-rmspe:0.136351\n",
      "[297]\ttrain-rmse:0.088864\teval-rmse:0.126455\ttrain-rmspe:0.101954\teval-rmspe:0.136328\n",
      "[298]\ttrain-rmse:0.088762\teval-rmse:0.126407\ttrain-rmspe:0.101828\teval-rmspe:0.136283\n",
      "[299]\ttrain-rmse:0.088692\teval-rmse:0.126432\ttrain-rmspe:0.101726\teval-rmspe:0.136316\n",
      "[300]\ttrain-rmse:0.088583\teval-rmse:0.126391\ttrain-rmspe:0.101523\teval-rmspe:0.136273\n",
      "[301]\ttrain-rmse:0.088514\teval-rmse:0.126372\ttrain-rmspe:0.101454\teval-rmspe:0.136251\n",
      "[302]\ttrain-rmse:0.088404\teval-rmse:0.12634\ttrain-rmspe:0.100916\teval-rmspe:0.136221\n",
      "[303]\ttrain-rmse:0.08836\teval-rmse:0.126351\ttrain-rmspe:0.100868\teval-rmspe:0.136242\n",
      "[304]\ttrain-rmse:0.088309\teval-rmse:0.126353\ttrain-rmspe:0.100815\teval-rmspe:0.136243\n",
      "[305]\ttrain-rmse:0.088242\teval-rmse:0.126286\ttrain-rmspe:0.100728\teval-rmspe:0.136164\n",
      "[306]\ttrain-rmse:0.088165\teval-rmse:0.126264\ttrain-rmspe:0.100639\teval-rmspe:0.136156\n",
      "[307]\ttrain-rmse:0.088056\teval-rmse:0.126216\ttrain-rmspe:0.100525\teval-rmspe:0.136099\n",
      "[308]\ttrain-rmse:0.088003\teval-rmse:0.126182\ttrain-rmspe:0.100462\teval-rmspe:0.136059\n",
      "[309]\ttrain-rmse:0.087896\teval-rmse:0.126143\ttrain-rmspe:0.100352\teval-rmspe:0.136019\n",
      "[310]\ttrain-rmse:0.087831\teval-rmse:0.126124\ttrain-rmspe:0.100271\teval-rmspe:0.135999\n",
      "[311]\ttrain-rmse:0.087802\teval-rmse:0.126106\ttrain-rmspe:0.100233\teval-rmspe:0.135977\n",
      "[312]\ttrain-rmse:0.087742\teval-rmse:0.126067\ttrain-rmspe:0.100151\teval-rmspe:0.135944\n",
      "[313]\ttrain-rmse:0.087677\teval-rmse:0.126066\ttrain-rmspe:0.100087\teval-rmspe:0.135935\n",
      "[314]\ttrain-rmse:0.087619\teval-rmse:0.126052\ttrain-rmspe:0.099904\teval-rmspe:0.135917\n",
      "[315]\ttrain-rmse:0.08749\teval-rmse:0.12606\ttrain-rmspe:0.099723\teval-rmspe:0.135942\n",
      "[316]\ttrain-rmse:0.087405\teval-rmse:0.126027\ttrain-rmspe:0.099625\teval-rmspe:0.135903\n",
      "[317]\ttrain-rmse:0.087386\teval-rmse:0.12601\ttrain-rmspe:0.099603\teval-rmspe:0.135875\n",
      "[318]\ttrain-rmse:0.087334\teval-rmse:0.126098\ttrain-rmspe:0.099545\teval-rmspe:0.136008\n",
      "[319]\ttrain-rmse:0.087273\teval-rmse:0.12611\ttrain-rmspe:0.099453\teval-rmspe:0.136019\n",
      "[320]\ttrain-rmse:0.087189\teval-rmse:0.126076\ttrain-rmspe:0.099364\teval-rmspe:0.135985\n",
      "[321]\ttrain-rmse:0.087099\teval-rmse:0.126041\ttrain-rmspe:0.099197\teval-rmspe:0.135949\n",
      "[322]\ttrain-rmse:0.087049\teval-rmse:0.126032\ttrain-rmspe:0.098809\teval-rmspe:0.135942\n",
      "[323]\ttrain-rmse:0.086952\teval-rmse:0.126023\ttrain-rmspe:0.098609\teval-rmspe:0.135938\n",
      "[324]\ttrain-rmse:0.086874\teval-rmse:0.125974\ttrain-rmspe:0.098523\teval-rmspe:0.135881\n",
      "[325]\ttrain-rmse:0.086823\teval-rmse:0.125957\ttrain-rmspe:0.09846\teval-rmspe:0.135863\n",
      "[326]\ttrain-rmse:0.086765\teval-rmse:0.125961\ttrain-rmspe:0.098401\teval-rmspe:0.135865\n",
      "[327]\ttrain-rmse:0.086692\teval-rmse:0.125985\ttrain-rmspe:0.098324\teval-rmspe:0.135899\n",
      "[328]\ttrain-rmse:0.086607\teval-rmse:0.125974\ttrain-rmspe:0.098252\teval-rmspe:0.13587\n",
      "[329]\ttrain-rmse:0.086484\teval-rmse:0.12591\ttrain-rmspe:0.098128\teval-rmspe:0.135805\n",
      "[330]\ttrain-rmse:0.0864\teval-rmse:0.125877\ttrain-rmspe:0.098012\teval-rmspe:0.135766\n",
      "[331]\ttrain-rmse:0.08633\teval-rmse:0.125848\ttrain-rmspe:0.097888\teval-rmspe:0.135733\n",
      "[332]\ttrain-rmse:0.086259\teval-rmse:0.12581\ttrain-rmspe:0.097811\teval-rmspe:0.135686\n",
      "[333]\ttrain-rmse:0.0862\teval-rmse:0.125771\ttrain-rmspe:0.097673\teval-rmspe:0.135636\n",
      "[334]\ttrain-rmse:0.086091\teval-rmse:0.12573\ttrain-rmspe:0.097555\teval-rmspe:0.135589\n",
      "[335]\ttrain-rmse:0.08603\teval-rmse:0.125703\ttrain-rmspe:0.09749\teval-rmspe:0.135556\n",
      "[336]\ttrain-rmse:0.08598\teval-rmse:0.125702\ttrain-rmspe:0.097369\teval-rmspe:0.135559\n",
      "[337]\ttrain-rmse:0.085929\teval-rmse:0.125666\ttrain-rmspe:0.097315\teval-rmspe:0.135526\n",
      "[338]\ttrain-rmse:0.08587\teval-rmse:0.125678\ttrain-rmspe:0.097231\teval-rmspe:0.13554\n",
      "[339]\ttrain-rmse:0.085812\teval-rmse:0.125674\ttrain-rmspe:0.097153\teval-rmspe:0.13553\n",
      "[340]\ttrain-rmse:0.085762\teval-rmse:0.125661\ttrain-rmspe:0.097105\teval-rmspe:0.135515\n",
      "[341]\ttrain-rmse:0.0857\teval-rmse:0.125635\ttrain-rmspe:0.097039\teval-rmspe:0.135463\n",
      "[342]\ttrain-rmse:0.08563\teval-rmse:0.125613\ttrain-rmspe:0.096962\teval-rmspe:0.13544\n",
      "[343]\ttrain-rmse:0.085531\teval-rmse:0.125582\ttrain-rmspe:0.096864\teval-rmspe:0.135408\n",
      "[344]\ttrain-rmse:0.085473\teval-rmse:0.125559\ttrain-rmspe:0.096809\teval-rmspe:0.135377\n",
      "[345]\ttrain-rmse:0.085338\teval-rmse:0.125487\ttrain-rmspe:0.096664\teval-rmspe:0.13531\n",
      "[346]\ttrain-rmse:0.085294\teval-rmse:0.125498\ttrain-rmspe:0.096623\teval-rmspe:0.135314\n",
      "[347]\ttrain-rmse:0.085235\teval-rmse:0.125376\ttrain-rmspe:0.096532\teval-rmspe:0.135134\n",
      "[348]\ttrain-rmse:0.085194\teval-rmse:0.125492\ttrain-rmspe:0.096481\teval-rmspe:0.135276\n",
      "[349]\ttrain-rmse:0.085117\teval-rmse:0.125455\ttrain-rmspe:0.096404\teval-rmspe:0.135238\n",
      "[350]\ttrain-rmse:0.085035\teval-rmse:0.125419\ttrain-rmspe:0.096325\teval-rmspe:0.1352\n",
      "[351]\ttrain-rmse:0.08495\teval-rmse:0.125263\ttrain-rmspe:0.096233\teval-rmspe:0.134989\n",
      "[352]\ttrain-rmse:0.084898\teval-rmse:0.125255\ttrain-rmspe:0.096177\teval-rmspe:0.134985\n",
      "[353]\ttrain-rmse:0.08485\teval-rmse:0.125268\ttrain-rmspe:0.096124\teval-rmspe:0.135017\n",
      "[354]\ttrain-rmse:0.084786\teval-rmse:0.12525\ttrain-rmspe:0.096059\teval-rmspe:0.135\n",
      "[355]\ttrain-rmse:0.084697\teval-rmse:0.125222\ttrain-rmspe:0.095942\teval-rmspe:0.134973\n",
      "[356]\ttrain-rmse:0.084671\teval-rmse:0.125218\ttrain-rmspe:0.095917\teval-rmspe:0.134967\n",
      "[357]\ttrain-rmse:0.084603\teval-rmse:0.12522\ttrain-rmspe:0.095782\teval-rmspe:0.134971\n",
      "[358]\ttrain-rmse:0.084534\teval-rmse:0.125177\ttrain-rmspe:0.095702\teval-rmspe:0.134929\n",
      "[359]\ttrain-rmse:0.084476\teval-rmse:0.125167\ttrain-rmspe:0.095643\teval-rmspe:0.134908\n",
      "[360]\ttrain-rmse:0.084434\teval-rmse:0.125155\ttrain-rmspe:0.095602\teval-rmspe:0.134905\n",
      "[361]\ttrain-rmse:0.084368\teval-rmse:0.125013\ttrain-rmspe:0.095478\teval-rmspe:0.134706\n",
      "[362]\ttrain-rmse:0.084292\teval-rmse:0.124988\ttrain-rmspe:0.095402\teval-rmspe:0.134685\n",
      "[363]\ttrain-rmse:0.084272\teval-rmse:0.124984\ttrain-rmspe:0.095379\teval-rmspe:0.134664\n",
      "[364]\ttrain-rmse:0.084232\teval-rmse:0.124969\ttrain-rmspe:0.095314\teval-rmspe:0.134643\n",
      "[365]\ttrain-rmse:0.084198\teval-rmse:0.124962\ttrain-rmspe:0.095282\teval-rmspe:0.134635\n",
      "[366]\ttrain-rmse:0.084126\teval-rmse:0.124938\ttrain-rmspe:0.0952\teval-rmspe:0.134616\n",
      "[367]\ttrain-rmse:0.08408\teval-rmse:0.124937\ttrain-rmspe:0.095121\teval-rmspe:0.134615\n",
      "[368]\ttrain-rmse:0.084046\teval-rmse:0.124916\ttrain-rmspe:0.095087\teval-rmspe:0.134589\n",
      "[369]\ttrain-rmse:0.084001\teval-rmse:0.125072\ttrain-rmspe:0.095045\teval-rmspe:0.134774\n",
      "[370]\ttrain-rmse:0.083951\teval-rmse:0.12508\ttrain-rmspe:0.094972\teval-rmspe:0.134785\n",
      "[371]\ttrain-rmse:0.08391\teval-rmse:0.125087\ttrain-rmspe:0.094932\teval-rmspe:0.134797\n",
      "[372]\ttrain-rmse:0.083864\teval-rmse:0.12508\ttrain-rmspe:0.094889\teval-rmspe:0.134794\n",
      "[373]\ttrain-rmse:0.083812\teval-rmse:0.125089\ttrain-rmspe:0.094781\teval-rmspe:0.134817\n",
      "[374]\ttrain-rmse:0.083773\teval-rmse:0.12507\ttrain-rmspe:0.094741\teval-rmspe:0.134793\n",
      "[375]\ttrain-rmse:0.083712\teval-rmse:0.125037\ttrain-rmspe:0.094682\teval-rmspe:0.134755\n",
      "[376]\ttrain-rmse:0.083643\teval-rmse:0.125026\ttrain-rmspe:0.094342\teval-rmspe:0.134744\n",
      "[377]\ttrain-rmse:0.083595\teval-rmse:0.125007\ttrain-rmspe:0.094298\teval-rmspe:0.134723\n",
      "[378]\ttrain-rmse:0.08355\teval-rmse:0.124985\ttrain-rmspe:0.094245\teval-rmspe:0.134695\n",
      "[379]\ttrain-rmse:0.083502\teval-rmse:0.124974\ttrain-rmspe:0.094202\teval-rmspe:0.134683\n",
      "[380]\ttrain-rmse:0.083481\teval-rmse:0.124972\ttrain-rmspe:0.094162\teval-rmspe:0.134684\n",
      "[381]\ttrain-rmse:0.083393\teval-rmse:0.124955\ttrain-rmspe:0.093984\teval-rmspe:0.134676\n",
      "[382]\ttrain-rmse:0.083344\teval-rmse:0.12494\ttrain-rmspe:0.09389\teval-rmspe:0.134663\n",
      "[383]\ttrain-rmse:0.083312\teval-rmse:0.124952\ttrain-rmspe:0.093855\teval-rmspe:0.134678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[384]\ttrain-rmse:0.083262\teval-rmse:0.124951\ttrain-rmspe:0.0938\teval-rmspe:0.134681\n",
      "[385]\ttrain-rmse:0.083202\teval-rmse:0.124932\ttrain-rmspe:0.093741\teval-rmspe:0.134673\n",
      "[386]\ttrain-rmse:0.083138\teval-rmse:0.124941\ttrain-rmspe:0.093634\teval-rmspe:0.134699\n",
      "[387]\ttrain-rmse:0.0831\teval-rmse:0.124936\ttrain-rmspe:0.093596\teval-rmspe:0.134698\n",
      "[388]\ttrain-rmse:0.082995\teval-rmse:0.124906\ttrain-rmspe:0.093487\teval-rmspe:0.134663\n",
      "[389]\ttrain-rmse:0.082947\teval-rmse:0.12491\ttrain-rmspe:0.093429\teval-rmspe:0.134669\n",
      "[390]\ttrain-rmse:0.082915\teval-rmse:0.124906\ttrain-rmspe:0.093401\teval-rmspe:0.13467\n",
      "[391]\ttrain-rmse:0.082886\teval-rmse:0.124912\ttrain-rmspe:0.093373\teval-rmspe:0.134679\n",
      "[392]\ttrain-rmse:0.082835\teval-rmse:0.124905\ttrain-rmspe:0.093324\teval-rmspe:0.134674\n",
      "[393]\ttrain-rmse:0.082794\teval-rmse:0.124925\ttrain-rmspe:0.093286\teval-rmspe:0.134708\n",
      "[394]\ttrain-rmse:0.082749\teval-rmse:0.124916\ttrain-rmspe:0.09323\teval-rmspe:0.134698\n",
      "[395]\ttrain-rmse:0.082704\teval-rmse:0.124899\ttrain-rmspe:0.093172\teval-rmspe:0.134683\n",
      "[396]\ttrain-rmse:0.082614\teval-rmse:0.124863\ttrain-rmspe:0.093075\teval-rmspe:0.134644\n",
      "[397]\ttrain-rmse:0.08256\teval-rmse:0.124812\ttrain-rmspe:0.093013\teval-rmspe:0.134594\n",
      "[398]\ttrain-rmse:0.082492\teval-rmse:0.124782\ttrain-rmspe:0.092917\teval-rmspe:0.134563\n",
      "[399]\ttrain-rmse:0.082419\teval-rmse:0.124788\ttrain-rmspe:0.092837\teval-rmspe:0.134579\n",
      "[400]\ttrain-rmse:0.082356\teval-rmse:0.124775\ttrain-rmspe:0.092747\teval-rmspe:0.134568\n",
      "[401]\ttrain-rmse:0.082321\teval-rmse:0.124745\ttrain-rmspe:0.092698\teval-rmspe:0.134522\n",
      "[402]\ttrain-rmse:0.082267\teval-rmse:0.124741\ttrain-rmspe:0.092632\teval-rmspe:0.13452\n",
      "[403]\ttrain-rmse:0.082219\teval-rmse:0.124728\ttrain-rmspe:0.09258\teval-rmspe:0.134507\n",
      "[404]\ttrain-rmse:0.082165\teval-rmse:0.124724\ttrain-rmspe:0.092519\teval-rmspe:0.134504\n",
      "[405]\ttrain-rmse:0.082099\teval-rmse:0.124727\ttrain-rmspe:0.09245\teval-rmspe:0.134505\n",
      "[406]\ttrain-rmse:0.082063\teval-rmse:0.124714\ttrain-rmspe:0.092384\teval-rmspe:0.134486\n",
      "[407]\ttrain-rmse:0.081963\teval-rmse:0.124669\ttrain-rmspe:0.092287\teval-rmspe:0.134438\n",
      "[408]\ttrain-rmse:0.081931\teval-rmse:0.124678\ttrain-rmspe:0.092233\teval-rmspe:0.134459\n",
      "[409]\ttrain-rmse:0.081891\teval-rmse:0.124669\ttrain-rmspe:0.092166\teval-rmspe:0.13445\n",
      "[410]\ttrain-rmse:0.08183\teval-rmse:0.124651\ttrain-rmspe:0.092073\teval-rmspe:0.134438\n",
      "[411]\ttrain-rmse:0.081808\teval-rmse:0.124668\ttrain-rmspe:0.09205\teval-rmspe:0.134469\n",
      "[412]\ttrain-rmse:0.081775\teval-rmse:0.124661\ttrain-rmspe:0.092003\teval-rmspe:0.134467\n",
      "[413]\ttrain-rmse:0.081743\teval-rmse:0.124657\ttrain-rmspe:0.091951\teval-rmspe:0.134455\n",
      "[414]\ttrain-rmse:0.081701\teval-rmse:0.124642\ttrain-rmspe:0.091907\teval-rmspe:0.134436\n",
      "[415]\ttrain-rmse:0.081634\teval-rmse:0.124607\ttrain-rmspe:0.09183\teval-rmspe:0.134396\n",
      "[416]\ttrain-rmse:0.081556\teval-rmse:0.124581\ttrain-rmspe:0.091755\teval-rmspe:0.134372\n",
      "[417]\ttrain-rmse:0.081541\teval-rmse:0.124587\ttrain-rmspe:0.09174\teval-rmspe:0.134374\n",
      "[418]\ttrain-rmse:0.081509\teval-rmse:0.124561\ttrain-rmspe:0.091698\teval-rmspe:0.13435\n",
      "[419]\ttrain-rmse:0.08148\teval-rmse:0.124546\ttrain-rmspe:0.091673\teval-rmspe:0.134331\n",
      "[420]\ttrain-rmse:0.081423\teval-rmse:0.124539\ttrain-rmspe:0.091614\teval-rmspe:0.134327\n",
      "[421]\ttrain-rmse:0.081368\teval-rmse:0.124529\ttrain-rmspe:0.091557\teval-rmspe:0.134323\n",
      "[422]\ttrain-rmse:0.081335\teval-rmse:0.124515\ttrain-rmspe:0.091526\teval-rmspe:0.134315\n",
      "[423]\ttrain-rmse:0.081293\teval-rmse:0.12449\ttrain-rmspe:0.091337\teval-rmspe:0.134287\n",
      "[424]\ttrain-rmse:0.081253\teval-rmse:0.124493\ttrain-rmspe:0.091289\teval-rmspe:0.134291\n",
      "[425]\ttrain-rmse:0.081205\teval-rmse:0.124467\ttrain-rmspe:0.091241\teval-rmspe:0.134256\n",
      "[426]\ttrain-rmse:0.081158\teval-rmse:0.124454\ttrain-rmspe:0.091192\teval-rmspe:0.134245\n",
      "[427]\ttrain-rmse:0.081116\teval-rmse:0.124446\ttrain-rmspe:0.091117\teval-rmspe:0.134236\n",
      "[428]\ttrain-rmse:0.081065\teval-rmse:0.124417\ttrain-rmspe:0.091065\teval-rmspe:0.134206\n",
      "[429]\ttrain-rmse:0.081035\teval-rmse:0.124418\ttrain-rmspe:0.091032\teval-rmspe:0.134213\n",
      "[430]\ttrain-rmse:0.080961\teval-rmse:0.124397\ttrain-rmspe:0.09093\teval-rmspe:0.134196\n",
      "[431]\ttrain-rmse:0.080908\teval-rmse:0.124396\ttrain-rmspe:0.090828\teval-rmspe:0.134194\n",
      "[432]\ttrain-rmse:0.080874\teval-rmse:0.124398\ttrain-rmspe:0.090788\teval-rmspe:0.134197\n",
      "[433]\ttrain-rmse:0.080816\teval-rmse:0.124378\ttrain-rmspe:0.090712\teval-rmspe:0.134174\n",
      "[434]\ttrain-rmse:0.080766\teval-rmse:0.124372\ttrain-rmspe:0.090638\teval-rmspe:0.134172\n",
      "[435]\ttrain-rmse:0.080719\teval-rmse:0.124355\ttrain-rmspe:0.09059\teval-rmspe:0.134153\n",
      "[436]\ttrain-rmse:0.080647\teval-rmse:0.124332\ttrain-rmspe:0.090507\teval-rmspe:0.134129\n",
      "[437]\ttrain-rmse:0.080603\teval-rmse:0.124334\ttrain-rmspe:0.090456\teval-rmspe:0.134141\n",
      "[438]\ttrain-rmse:0.080569\teval-rmse:0.124335\ttrain-rmspe:0.090391\teval-rmspe:0.134141\n",
      "[439]\ttrain-rmse:0.080524\teval-rmse:0.124308\ttrain-rmspe:0.089817\teval-rmspe:0.134082\n",
      "[440]\ttrain-rmse:0.08048\teval-rmse:0.124287\ttrain-rmspe:0.089763\teval-rmspe:0.134058\n",
      "[441]\ttrain-rmse:0.080378\teval-rmse:0.124247\ttrain-rmspe:0.089659\teval-rmspe:0.134019\n",
      "[442]\ttrain-rmse:0.080316\teval-rmse:0.124232\ttrain-rmspe:0.089592\teval-rmspe:0.134006\n",
      "[443]\ttrain-rmse:0.080284\teval-rmse:0.124221\ttrain-rmspe:0.089559\teval-rmspe:0.133992\n",
      "[444]\ttrain-rmse:0.080253\teval-rmse:0.124198\ttrain-rmspe:0.089528\teval-rmspe:0.133966\n",
      "[445]\ttrain-rmse:0.080214\teval-rmse:0.124226\ttrain-rmspe:0.089489\teval-rmspe:0.133986\n",
      "[446]\ttrain-rmse:0.080178\teval-rmse:0.12422\ttrain-rmspe:0.089446\teval-rmspe:0.133976\n",
      "[447]\ttrain-rmse:0.080117\teval-rmse:0.124207\ttrain-rmspe:0.089376\teval-rmspe:0.133974\n",
      "[448]\ttrain-rmse:0.080078\teval-rmse:0.124218\ttrain-rmspe:0.089318\teval-rmspe:0.134002\n",
      "[449]\ttrain-rmse:0.080046\teval-rmse:0.124198\ttrain-rmspe:0.089274\teval-rmspe:0.133981\n",
      "[450]\ttrain-rmse:0.080008\teval-rmse:0.124186\ttrain-rmspe:0.089226\teval-rmspe:0.133965\n",
      "[451]\ttrain-rmse:0.079973\teval-rmse:0.124164\ttrain-rmspe:0.089187\teval-rmspe:0.133928\n",
      "[452]\ttrain-rmse:0.07992\teval-rmse:0.124165\ttrain-rmspe:0.089113\teval-rmspe:0.133924\n",
      "[453]\ttrain-rmse:0.07986\teval-rmse:0.12419\ttrain-rmspe:0.089022\teval-rmspe:0.133959\n",
      "[454]\ttrain-rmse:0.079806\teval-rmse:0.12422\ttrain-rmspe:0.088932\teval-rmspe:0.134013\n",
      "[455]\ttrain-rmse:0.079763\teval-rmse:0.124208\ttrain-rmspe:0.088871\teval-rmspe:0.133998\n",
      "[456]\ttrain-rmse:0.079722\teval-rmse:0.124178\ttrain-rmspe:0.088821\teval-rmspe:0.133963\n",
      "[457]\ttrain-rmse:0.079671\teval-rmse:0.124169\ttrain-rmspe:0.088741\teval-rmspe:0.133955\n",
      "[458]\ttrain-rmse:0.079616\teval-rmse:0.124146\ttrain-rmspe:0.088569\teval-rmspe:0.133937\n",
      "[459]\ttrain-rmse:0.079575\teval-rmse:0.12413\ttrain-rmspe:0.088511\teval-rmspe:0.133918\n",
      "[460]\ttrain-rmse:0.079539\teval-rmse:0.124134\ttrain-rmspe:0.088474\teval-rmspe:0.133926\n",
      "[461]\ttrain-rmse:0.079505\teval-rmse:0.124131\ttrain-rmspe:0.088416\teval-rmspe:0.133922\n",
      "[462]\ttrain-rmse:0.079456\teval-rmse:0.12413\ttrain-rmspe:0.088353\teval-rmspe:0.133925\n",
      "[463]\ttrain-rmse:0.079401\teval-rmse:0.124116\ttrain-rmspe:0.088282\teval-rmspe:0.13391\n",
      "[464]\ttrain-rmse:0.079372\teval-rmse:0.12412\ttrain-rmspe:0.088251\teval-rmspe:0.133918\n",
      "[465]\ttrain-rmse:0.079331\teval-rmse:0.124121\ttrain-rmspe:0.08819\teval-rmspe:0.133921\n",
      "[466]\ttrain-rmse:0.07929\teval-rmse:0.124102\ttrain-rmspe:0.088144\teval-rmspe:0.133905\n",
      "[467]\ttrain-rmse:0.079256\teval-rmse:0.124098\ttrain-rmspe:0.088108\teval-rmspe:0.133903\n",
      "[468]\ttrain-rmse:0.079217\teval-rmse:0.124076\ttrain-rmspe:0.088065\teval-rmspe:0.133879\n",
      "[469]\ttrain-rmse:0.079178\teval-rmse:0.124088\ttrain-rmspe:0.088001\teval-rmspe:0.133894\n",
      "[470]\ttrain-rmse:0.079144\teval-rmse:0.124072\ttrain-rmspe:0.087971\teval-rmspe:0.133879\n",
      "[471]\ttrain-rmse:0.0791\teval-rmse:0.124069\ttrain-rmspe:0.087911\teval-rmspe:0.133877\n",
      "[472]\ttrain-rmse:0.079045\teval-rmse:0.124053\ttrain-rmspe:0.087856\teval-rmspe:0.133858\n",
      "[473]\ttrain-rmse:0.079006\teval-rmse:0.124046\ttrain-rmspe:0.087818\teval-rmspe:0.133847\n",
      "[474]\ttrain-rmse:0.078977\teval-rmse:0.124038\ttrain-rmspe:0.087782\teval-rmspe:0.13384\n",
      "[475]\ttrain-rmse:0.078925\teval-rmse:0.124027\ttrain-rmspe:0.087706\teval-rmspe:0.133834\n",
      "[476]\ttrain-rmse:0.078884\teval-rmse:0.124043\ttrain-rmspe:0.08765\teval-rmspe:0.133864\n",
      "[477]\ttrain-rmse:0.078841\teval-rmse:0.124027\ttrain-rmspe:0.087606\teval-rmspe:0.133843\n",
      "[478]\ttrain-rmse:0.078803\teval-rmse:0.124028\ttrain-rmspe:0.087569\teval-rmspe:0.133847\n",
      "[479]\ttrain-rmse:0.078767\teval-rmse:0.124024\ttrain-rmspe:0.087521\teval-rmspe:0.133838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[480]\ttrain-rmse:0.078731\teval-rmse:0.124015\ttrain-rmspe:0.08745\teval-rmspe:0.133834\n",
      "[481]\ttrain-rmse:0.078696\teval-rmse:0.124007\ttrain-rmspe:0.087388\teval-rmspe:0.133824\n",
      "[482]\ttrain-rmse:0.078655\teval-rmse:0.124\ttrain-rmspe:0.087318\teval-rmspe:0.133818\n",
      "[483]\ttrain-rmse:0.078611\teval-rmse:0.124027\ttrain-rmspe:0.087271\teval-rmspe:0.133831\n",
      "[484]\ttrain-rmse:0.078575\teval-rmse:0.124018\ttrain-rmspe:0.087232\teval-rmspe:0.133817\n",
      "[485]\ttrain-rmse:0.078536\teval-rmse:0.124011\ttrain-rmspe:0.087193\teval-rmspe:0.133816\n",
      "[486]\ttrain-rmse:0.078498\teval-rmse:0.124003\ttrain-rmspe:0.087143\teval-rmspe:0.133811\n",
      "[487]\ttrain-rmse:0.078468\teval-rmse:0.124005\ttrain-rmspe:0.087108\teval-rmspe:0.133811\n",
      "[488]\ttrain-rmse:0.078433\teval-rmse:0.12401\ttrain-rmspe:0.086933\teval-rmspe:0.133823\n",
      "[489]\ttrain-rmse:0.078382\teval-rmse:0.123977\ttrain-rmspe:0.086884\teval-rmspe:0.133774\n",
      "[490]\ttrain-rmse:0.078324\teval-rmse:0.123948\ttrain-rmspe:0.086826\teval-rmspe:0.133743\n",
      "[491]\ttrain-rmse:0.078272\teval-rmse:0.123943\ttrain-rmspe:0.086731\teval-rmspe:0.133738\n",
      "[492]\ttrain-rmse:0.07825\teval-rmse:0.123939\ttrain-rmspe:0.086701\teval-rmspe:0.133732\n",
      "[493]\ttrain-rmse:0.078223\teval-rmse:0.123937\ttrain-rmspe:0.086667\teval-rmspe:0.133733\n",
      "[494]\ttrain-rmse:0.078169\teval-rmse:0.123919\ttrain-rmspe:0.086599\teval-rmspe:0.133707\n",
      "[495]\ttrain-rmse:0.078138\teval-rmse:0.123923\ttrain-rmspe:0.086569\teval-rmspe:0.133714\n",
      "[496]\ttrain-rmse:0.078112\teval-rmse:0.123904\ttrain-rmspe:0.086529\teval-rmspe:0.133694\n",
      "[497]\ttrain-rmse:0.078068\teval-rmse:0.123922\ttrain-rmspe:0.086465\teval-rmspe:0.133742\n",
      "[498]\ttrain-rmse:0.078034\teval-rmse:0.123906\ttrain-rmspe:0.08629\teval-rmspe:0.133726\n",
      "[499]\ttrain-rmse:0.078011\teval-rmse:0.123897\ttrain-rmspe:0.086265\teval-rmspe:0.133714\n",
      "[500]\ttrain-rmse:0.077974\teval-rmse:0.123888\ttrain-rmspe:0.086222\teval-rmspe:0.1337\n",
      "[501]\ttrain-rmse:0.077941\teval-rmse:0.123891\ttrain-rmspe:0.08612\teval-rmspe:0.133696\n",
      "[502]\ttrain-rmse:0.0779\teval-rmse:0.123892\ttrain-rmspe:0.086071\teval-rmspe:0.133703\n",
      "[503]\ttrain-rmse:0.077869\teval-rmse:0.123891\ttrain-rmspe:0.086019\teval-rmspe:0.133701\n",
      "[504]\ttrain-rmse:0.077825\teval-rmse:0.123885\ttrain-rmspe:0.08596\teval-rmspe:0.133695\n",
      "[505]\ttrain-rmse:0.077792\teval-rmse:0.123907\ttrain-rmspe:0.08592\teval-rmspe:0.133722\n",
      "[506]\ttrain-rmse:0.077754\teval-rmse:0.123916\ttrain-rmspe:0.085879\teval-rmspe:0.133717\n",
      "[507]\ttrain-rmse:0.077701\teval-rmse:0.123899\ttrain-rmspe:0.085823\teval-rmspe:0.133691\n",
      "[508]\ttrain-rmse:0.077672\teval-rmse:0.123889\ttrain-rmspe:0.085786\teval-rmspe:0.133677\n",
      "[509]\ttrain-rmse:0.077646\teval-rmse:0.123886\ttrain-rmspe:0.085753\teval-rmspe:0.133677\n",
      "[510]\ttrain-rmse:0.077604\teval-rmse:0.123873\ttrain-rmspe:0.085695\teval-rmspe:0.133663\n",
      "[511]\ttrain-rmse:0.077546\teval-rmse:0.123846\ttrain-rmspe:0.085623\teval-rmspe:0.133633\n",
      "[512]\ttrain-rmse:0.0775\teval-rmse:0.123861\ttrain-rmspe:0.085541\teval-rmspe:0.133645\n",
      "[513]\ttrain-rmse:0.077468\teval-rmse:0.123872\ttrain-rmspe:0.085504\teval-rmspe:0.133657\n",
      "[514]\ttrain-rmse:0.07744\teval-rmse:0.12387\ttrain-rmspe:0.085477\teval-rmspe:0.133654\n",
      "[515]\ttrain-rmse:0.077395\teval-rmse:0.123854\ttrain-rmspe:0.08542\teval-rmspe:0.133639\n",
      "[516]\ttrain-rmse:0.077366\teval-rmse:0.123869\ttrain-rmspe:0.085389\teval-rmspe:0.133654\n",
      "[517]\ttrain-rmse:0.077345\teval-rmse:0.123868\ttrain-rmspe:0.085365\teval-rmspe:0.133658\n",
      "[518]\ttrain-rmse:0.077306\teval-rmse:0.123855\ttrain-rmspe:0.085316\teval-rmspe:0.133642\n",
      "[519]\ttrain-rmse:0.077278\teval-rmse:0.123857\ttrain-rmspe:0.085283\teval-rmspe:0.133655\n",
      "[520]\ttrain-rmse:0.077244\teval-rmse:0.123864\ttrain-rmspe:0.085251\teval-rmspe:0.133653\n",
      "[521]\ttrain-rmse:0.077215\teval-rmse:0.123862\ttrain-rmspe:0.085215\teval-rmspe:0.133655\n",
      "[522]\ttrain-rmse:0.077197\teval-rmse:0.123862\ttrain-rmspe:0.085192\teval-rmspe:0.133651\n",
      "[523]\ttrain-rmse:0.077151\teval-rmse:0.123849\ttrain-rmspe:0.085138\teval-rmspe:0.133623\n",
      "[524]\ttrain-rmse:0.077133\teval-rmse:0.123841\ttrain-rmspe:0.08512\teval-rmspe:0.133616\n",
      "[525]\ttrain-rmse:0.077114\teval-rmse:0.123827\ttrain-rmspe:0.085101\teval-rmspe:0.1336\n",
      "[526]\ttrain-rmse:0.077063\teval-rmse:0.123814\ttrain-rmspe:0.084976\teval-rmspe:0.133587\n",
      "[527]\ttrain-rmse:0.07702\teval-rmse:0.123815\ttrain-rmspe:0.084924\teval-rmspe:0.133588\n",
      "[528]\ttrain-rmse:0.076967\teval-rmse:0.123797\ttrain-rmspe:0.08485\teval-rmspe:0.133566\n",
      "[529]\ttrain-rmse:0.076931\teval-rmse:0.123788\ttrain-rmspe:0.084804\teval-rmspe:0.133549\n",
      "[530]\ttrain-rmse:0.07691\teval-rmse:0.123806\ttrain-rmspe:0.084781\teval-rmspe:0.133563\n",
      "[531]\ttrain-rmse:0.076881\teval-rmse:0.123796\ttrain-rmspe:0.08475\teval-rmspe:0.133554\n",
      "[532]\ttrain-rmse:0.076839\teval-rmse:0.123786\ttrain-rmspe:0.084693\teval-rmspe:0.133542\n",
      "[533]\ttrain-rmse:0.076791\teval-rmse:0.123767\ttrain-rmspe:0.084637\teval-rmspe:0.133527\n",
      "[534]\ttrain-rmse:0.07676\teval-rmse:0.123766\ttrain-rmspe:0.084602\teval-rmspe:0.133527\n",
      "[535]\ttrain-rmse:0.076713\teval-rmse:0.123773\ttrain-rmspe:0.084543\teval-rmspe:0.133533\n",
      "[536]\ttrain-rmse:0.076683\teval-rmse:0.123772\ttrain-rmspe:0.084504\teval-rmspe:0.13353\n",
      "[537]\ttrain-rmse:0.07664\teval-rmse:0.12376\ttrain-rmspe:0.084428\teval-rmspe:0.133509\n",
      "[538]\ttrain-rmse:0.076623\teval-rmse:0.12376\ttrain-rmspe:0.084411\teval-rmspe:0.133512\n",
      "[539]\ttrain-rmse:0.076603\teval-rmse:0.123798\ttrain-rmspe:0.084391\teval-rmspe:0.133563\n",
      "[540]\ttrain-rmse:0.076572\teval-rmse:0.123788\ttrain-rmspe:0.084348\teval-rmspe:0.133549\n",
      "[541]\ttrain-rmse:0.076549\teval-rmse:0.123786\ttrain-rmspe:0.084318\teval-rmspe:0.133546\n",
      "[542]\ttrain-rmse:0.076512\teval-rmse:0.123783\ttrain-rmspe:0.084273\teval-rmspe:0.133542\n",
      "[543]\ttrain-rmse:0.076476\teval-rmse:0.123768\ttrain-rmspe:0.084232\teval-rmspe:0.13351\n",
      "[544]\ttrain-rmse:0.076456\teval-rmse:0.123761\ttrain-rmspe:0.084208\teval-rmspe:0.133518\n",
      "[545]\ttrain-rmse:0.076378\teval-rmse:0.123725\ttrain-rmspe:0.084094\teval-rmspe:0.133474\n",
      "[546]\ttrain-rmse:0.076352\teval-rmse:0.123728\ttrain-rmspe:0.084066\teval-rmspe:0.133478\n",
      "[547]\ttrain-rmse:0.076332\teval-rmse:0.123722\ttrain-rmspe:0.08404\teval-rmspe:0.133468\n",
      "[548]\ttrain-rmse:0.076292\teval-rmse:0.123728\ttrain-rmspe:0.083994\teval-rmspe:0.133476\n",
      "[549]\ttrain-rmse:0.076258\teval-rmse:0.12373\ttrain-rmspe:0.083873\teval-rmspe:0.133481\n",
      "[550]\ttrain-rmse:0.076234\teval-rmse:0.123725\ttrain-rmspe:0.083844\teval-rmspe:0.133476\n",
      "[551]\ttrain-rmse:0.076205\teval-rmse:0.123704\ttrain-rmspe:0.083814\teval-rmspe:0.133448\n",
      "[552]\ttrain-rmse:0.076176\teval-rmse:0.123719\ttrain-rmspe:0.083762\teval-rmspe:0.133468\n",
      "[553]\ttrain-rmse:0.076157\teval-rmse:0.123718\ttrain-rmspe:0.083743\teval-rmspe:0.133464\n",
      "[554]\ttrain-rmse:0.076135\teval-rmse:0.12372\ttrain-rmspe:0.08372\teval-rmspe:0.133468\n",
      "[555]\ttrain-rmse:0.076105\teval-rmse:0.123717\ttrain-rmspe:0.083689\teval-rmspe:0.13347\n",
      "[556]\ttrain-rmse:0.076086\teval-rmse:0.123696\ttrain-rmspe:0.083663\teval-rmspe:0.133443\n",
      "[557]\ttrain-rmse:0.07606\teval-rmse:0.123695\ttrain-rmspe:0.083633\teval-rmspe:0.133444\n",
      "[558]\ttrain-rmse:0.075997\teval-rmse:0.123678\ttrain-rmspe:0.083552\teval-rmspe:0.133428\n",
      "[559]\ttrain-rmse:0.075958\teval-rmse:0.123663\ttrain-rmspe:0.083515\teval-rmspe:0.133411\n",
      "[560]\ttrain-rmse:0.075933\teval-rmse:0.123673\ttrain-rmspe:0.083484\teval-rmspe:0.13343\n",
      "[561]\ttrain-rmse:0.075898\teval-rmse:0.123678\ttrain-rmspe:0.083445\teval-rmspe:0.133411\n",
      "[562]\ttrain-rmse:0.075838\teval-rmse:0.123668\ttrain-rmspe:0.083362\teval-rmspe:0.133402\n",
      "[563]\ttrain-rmse:0.075803\teval-rmse:0.123656\ttrain-rmspe:0.083322\teval-rmspe:0.133386\n",
      "[564]\ttrain-rmse:0.075761\teval-rmse:0.123655\ttrain-rmspe:0.083254\teval-rmspe:0.133386\n",
      "[565]\ttrain-rmse:0.075706\teval-rmse:0.123642\ttrain-rmspe:0.083021\teval-rmspe:0.133373\n",
      "[566]\ttrain-rmse:0.075659\teval-rmse:0.123635\ttrain-rmspe:0.082676\teval-rmspe:0.133366\n",
      "[567]\ttrain-rmse:0.075628\teval-rmse:0.123633\ttrain-rmspe:0.082644\teval-rmspe:0.13337\n",
      "[568]\ttrain-rmse:0.075609\teval-rmse:0.123626\ttrain-rmspe:0.082626\teval-rmspe:0.133363\n",
      "[569]\ttrain-rmse:0.075556\teval-rmse:0.123608\ttrain-rmspe:0.082571\teval-rmspe:0.133343\n",
      "[570]\ttrain-rmse:0.075529\teval-rmse:0.123601\ttrain-rmspe:0.082539\teval-rmspe:0.133334\n",
      "[571]\ttrain-rmse:0.075504\teval-rmse:0.123603\ttrain-rmspe:0.082487\teval-rmspe:0.13334\n",
      "[572]\ttrain-rmse:0.075466\teval-rmse:0.123587\ttrain-rmspe:0.082444\teval-rmspe:0.133326\n",
      "[573]\ttrain-rmse:0.07543\teval-rmse:0.123579\ttrain-rmspe:0.082402\teval-rmspe:0.133318\n",
      "[574]\ttrain-rmse:0.075394\teval-rmse:0.123563\ttrain-rmspe:0.08236\teval-rmspe:0.133301\n",
      "[575]\ttrain-rmse:0.07532\teval-rmse:0.123552\ttrain-rmspe:0.08227\teval-rmspe:0.133294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[576]\ttrain-rmse:0.075279\teval-rmse:0.123534\ttrain-rmspe:0.082203\teval-rmspe:0.133273\n",
      "[577]\ttrain-rmse:0.075241\teval-rmse:0.12353\ttrain-rmspe:0.082156\teval-rmspe:0.133267\n",
      "[578]\ttrain-rmse:0.07522\teval-rmse:0.12353\ttrain-rmspe:0.082133\teval-rmspe:0.133271\n",
      "[579]\ttrain-rmse:0.075167\teval-rmse:0.123512\ttrain-rmspe:0.082071\teval-rmspe:0.133255\n",
      "[580]\ttrain-rmse:0.075141\teval-rmse:0.123497\ttrain-rmspe:0.082042\teval-rmspe:0.133236\n",
      "[581]\ttrain-rmse:0.075127\teval-rmse:0.123497\ttrain-rmspe:0.082023\teval-rmspe:0.133234\n",
      "[582]\ttrain-rmse:0.075086\teval-rmse:0.123479\ttrain-rmspe:0.081976\teval-rmspe:0.133214\n",
      "[583]\ttrain-rmse:0.075056\teval-rmse:0.123475\ttrain-rmspe:0.081941\teval-rmspe:0.133208\n",
      "[584]\ttrain-rmse:0.07503\teval-rmse:0.123472\ttrain-rmspe:0.081917\teval-rmspe:0.133203\n",
      "[585]\ttrain-rmse:0.075\teval-rmse:0.123473\ttrain-rmspe:0.081886\teval-rmspe:0.133206\n",
      "[586]\ttrain-rmse:0.074956\teval-rmse:0.123485\ttrain-rmspe:0.08181\teval-rmspe:0.133224\n",
      "[587]\ttrain-rmse:0.074911\teval-rmse:0.123482\ttrain-rmspe:0.081712\teval-rmspe:0.133223\n",
      "[588]\ttrain-rmse:0.074872\teval-rmse:0.123477\ttrain-rmspe:0.081655\teval-rmspe:0.133224\n",
      "[589]\ttrain-rmse:0.074849\teval-rmse:0.123466\ttrain-rmspe:0.081632\teval-rmspe:0.133212\n",
      "[590]\ttrain-rmse:0.074808\teval-rmse:0.123453\ttrain-rmspe:0.081592\teval-rmspe:0.133197\n",
      "[591]\ttrain-rmse:0.074781\teval-rmse:0.123452\ttrain-rmspe:0.08153\teval-rmspe:0.133198\n",
      "[592]\ttrain-rmse:0.074749\teval-rmse:0.123454\ttrain-rmspe:0.081486\teval-rmspe:0.133208\n",
      "[593]\ttrain-rmse:0.074674\teval-rmse:0.123417\ttrain-rmspe:0.081397\teval-rmspe:0.133165\n",
      "[594]\ttrain-rmse:0.07464\teval-rmse:0.123418\ttrain-rmspe:0.081324\teval-rmspe:0.133166\n",
      "[595]\ttrain-rmse:0.074618\teval-rmse:0.123429\ttrain-rmspe:0.081301\teval-rmspe:0.133181\n",
      "[596]\ttrain-rmse:0.074596\teval-rmse:0.123425\ttrain-rmspe:0.081277\teval-rmspe:0.133179\n",
      "[597]\ttrain-rmse:0.074587\teval-rmse:0.123408\ttrain-rmspe:0.081259\teval-rmspe:0.133166\n",
      "[598]\ttrain-rmse:0.074558\teval-rmse:0.123398\ttrain-rmspe:0.081216\teval-rmspe:0.133146\n",
      "[599]\ttrain-rmse:0.074532\teval-rmse:0.123391\ttrain-rmspe:0.081189\teval-rmspe:0.133137\n",
      "[600]\ttrain-rmse:0.074495\teval-rmse:0.123392\ttrain-rmspe:0.081122\teval-rmspe:0.133134\n",
      "[601]\ttrain-rmse:0.074458\teval-rmse:0.123395\ttrain-rmspe:0.081067\teval-rmspe:0.133151\n",
      "[602]\ttrain-rmse:0.074424\teval-rmse:0.12339\ttrain-rmspe:0.081029\teval-rmspe:0.133143\n",
      "[603]\ttrain-rmse:0.074389\teval-rmse:0.123384\ttrain-rmspe:0.080988\teval-rmspe:0.133137\n",
      "[604]\ttrain-rmse:0.074369\teval-rmse:0.123374\ttrain-rmspe:0.080963\teval-rmspe:0.13312\n",
      "[605]\ttrain-rmse:0.074341\teval-rmse:0.123359\ttrain-rmspe:0.080932\teval-rmspe:0.133103\n",
      "[606]\ttrain-rmse:0.074307\teval-rmse:0.123357\ttrain-rmspe:0.080891\teval-rmspe:0.13311\n",
      "[607]\ttrain-rmse:0.074276\teval-rmse:0.123368\ttrain-rmspe:0.08084\teval-rmspe:0.133124\n",
      "[608]\ttrain-rmse:0.074244\teval-rmse:0.12336\ttrain-rmspe:0.080807\teval-rmspe:0.133119\n",
      "[609]\ttrain-rmse:0.074229\teval-rmse:0.123358\ttrain-rmspe:0.080793\teval-rmspe:0.13312\n",
      "[610]\ttrain-rmse:0.074185\teval-rmse:0.123315\ttrain-rmspe:0.080738\teval-rmspe:0.133049\n",
      "[611]\ttrain-rmse:0.074164\teval-rmse:0.123316\ttrain-rmspe:0.080717\teval-rmspe:0.133052\n",
      "[612]\ttrain-rmse:0.07414\teval-rmse:0.123317\ttrain-rmspe:0.080693\teval-rmspe:0.133052\n",
      "[613]\ttrain-rmse:0.074106\teval-rmse:0.1233\ttrain-rmspe:0.080653\teval-rmspe:0.133025\n",
      "[614]\ttrain-rmse:0.074067\teval-rmse:0.1233\ttrain-rmspe:0.080594\teval-rmspe:0.133027\n",
      "[615]\ttrain-rmse:0.074039\teval-rmse:0.12329\ttrain-rmspe:0.080537\teval-rmspe:0.133008\n",
      "[616]\ttrain-rmse:0.074015\teval-rmse:0.123298\ttrain-rmspe:0.08051\teval-rmspe:0.133026\n",
      "[617]\ttrain-rmse:0.073997\teval-rmse:0.123289\ttrain-rmspe:0.080489\teval-rmspe:0.133015\n",
      "[618]\ttrain-rmse:0.073953\teval-rmse:0.123276\ttrain-rmspe:0.08044\teval-rmspe:0.133009\n",
      "[619]\ttrain-rmse:0.073935\teval-rmse:0.123275\ttrain-rmspe:0.080419\teval-rmspe:0.133006\n",
      "[620]\ttrain-rmse:0.073891\teval-rmse:0.12327\ttrain-rmspe:0.080331\teval-rmspe:0.133\n",
      "[621]\ttrain-rmse:0.073871\teval-rmse:0.123259\ttrain-rmspe:0.080311\teval-rmspe:0.132985\n",
      "[622]\ttrain-rmse:0.073828\teval-rmse:0.123241\ttrain-rmspe:0.080261\teval-rmspe:0.132963\n",
      "[623]\ttrain-rmse:0.073812\teval-rmse:0.123237\ttrain-rmspe:0.080243\teval-rmspe:0.132952\n",
      "[624]\ttrain-rmse:0.073779\teval-rmse:0.123218\ttrain-rmspe:0.080203\teval-rmspe:0.132934\n",
      "[625]\ttrain-rmse:0.073762\teval-rmse:0.123219\ttrain-rmspe:0.080185\teval-rmspe:0.132936\n",
      "[626]\ttrain-rmse:0.073745\teval-rmse:0.123217\ttrain-rmspe:0.080167\teval-rmspe:0.132938\n",
      "[627]\ttrain-rmse:0.073711\teval-rmse:0.123212\ttrain-rmspe:0.080121\teval-rmspe:0.13293\n",
      "[628]\ttrain-rmse:0.073696\teval-rmse:0.12321\ttrain-rmspe:0.080106\teval-rmspe:0.13293\n",
      "[629]\ttrain-rmse:0.07368\teval-rmse:0.123194\ttrain-rmspe:0.08009\teval-rmspe:0.132893\n",
      "[630]\ttrain-rmse:0.073649\teval-rmse:0.123209\ttrain-rmspe:0.08005\teval-rmspe:0.132917\n",
      "[631]\ttrain-rmse:0.073607\teval-rmse:0.12319\ttrain-rmspe:0.080003\teval-rmspe:0.132901\n",
      "[632]\ttrain-rmse:0.073566\teval-rmse:0.123169\ttrain-rmspe:0.079939\teval-rmspe:0.132878\n",
      "[633]\ttrain-rmse:0.07353\teval-rmse:0.123152\ttrain-rmspe:0.079893\teval-rmspe:0.13286\n",
      "[634]\ttrain-rmse:0.073505\teval-rmse:0.123154\ttrain-rmspe:0.079858\teval-rmspe:0.132864\n",
      "[635]\ttrain-rmse:0.073474\teval-rmse:0.123136\ttrain-rmspe:0.079801\teval-rmspe:0.13283\n",
      "[636]\ttrain-rmse:0.073437\teval-rmse:0.123132\ttrain-rmspe:0.079715\teval-rmspe:0.132825\n",
      "[637]\ttrain-rmse:0.073413\teval-rmse:0.123123\ttrain-rmspe:0.079688\teval-rmspe:0.132815\n",
      "[638]\ttrain-rmse:0.07339\teval-rmse:0.123115\ttrain-rmspe:0.07966\teval-rmspe:0.132806\n",
      "[639]\ttrain-rmse:0.073345\teval-rmse:0.123079\ttrain-rmspe:0.079611\teval-rmspe:0.132766\n",
      "[640]\ttrain-rmse:0.073324\teval-rmse:0.123083\ttrain-rmspe:0.079589\teval-rmspe:0.132771\n",
      "[641]\ttrain-rmse:0.0733\teval-rmse:0.123099\ttrain-rmspe:0.079551\teval-rmspe:0.132782\n",
      "[642]\ttrain-rmse:0.073274\teval-rmse:0.123099\ttrain-rmspe:0.079501\teval-rmspe:0.132782\n",
      "[643]\ttrain-rmse:0.073249\teval-rmse:0.123079\ttrain-rmspe:0.079474\teval-rmspe:0.132758\n",
      "[644]\ttrain-rmse:0.073224\teval-rmse:0.123079\ttrain-rmspe:0.079445\teval-rmspe:0.132763\n",
      "[645]\ttrain-rmse:0.073209\teval-rmse:0.123085\ttrain-rmspe:0.079428\teval-rmspe:0.132775\n",
      "[646]\ttrain-rmse:0.073168\teval-rmse:0.123089\ttrain-rmspe:0.079363\teval-rmspe:0.132782\n",
      "[647]\ttrain-rmse:0.073138\teval-rmse:0.123085\ttrain-rmspe:0.079321\teval-rmspe:0.132776\n",
      "[648]\ttrain-rmse:0.073105\teval-rmse:0.123083\ttrain-rmspe:0.079274\teval-rmspe:0.132774\n",
      "[649]\ttrain-rmse:0.073081\teval-rmse:0.123084\ttrain-rmspe:0.079246\teval-rmspe:0.132776\n",
      "[650]\ttrain-rmse:0.073039\teval-rmse:0.123078\ttrain-rmspe:0.079201\teval-rmspe:0.132777\n",
      "[651]\ttrain-rmse:0.073014\teval-rmse:0.123081\ttrain-rmspe:0.079178\teval-rmspe:0.132781\n",
      "[652]\ttrain-rmse:0.072994\teval-rmse:0.123083\ttrain-rmspe:0.079157\teval-rmspe:0.132784\n",
      "[653]\ttrain-rmse:0.072971\teval-rmse:0.123078\ttrain-rmspe:0.079129\teval-rmspe:0.132778\n",
      "[654]\ttrain-rmse:0.072939\teval-rmse:0.123072\ttrain-rmspe:0.079084\teval-rmspe:0.132765\n",
      "[655]\ttrain-rmse:0.072905\teval-rmse:0.123071\ttrain-rmspe:0.079049\teval-rmspe:0.132766\n",
      "[656]\ttrain-rmse:0.072858\teval-rmse:0.123063\ttrain-rmspe:0.078988\teval-rmspe:0.132758\n",
      "[657]\ttrain-rmse:0.072842\teval-rmse:0.123091\ttrain-rmspe:0.078969\teval-rmspe:0.132795\n",
      "[658]\ttrain-rmse:0.072819\teval-rmse:0.123094\ttrain-rmspe:0.078945\teval-rmspe:0.132797\n",
      "[659]\ttrain-rmse:0.072789\teval-rmse:0.123083\ttrain-rmspe:0.078906\teval-rmspe:0.132786\n",
      "[660]\ttrain-rmse:0.072764\teval-rmse:0.123094\ttrain-rmspe:0.078867\teval-rmspe:0.132804\n",
      "[661]\ttrain-rmse:0.072737\teval-rmse:0.123094\ttrain-rmspe:0.078835\teval-rmspe:0.132805\n",
      "[662]\ttrain-rmse:0.072708\teval-rmse:0.123076\ttrain-rmspe:0.078802\teval-rmspe:0.13278\n",
      "[663]\ttrain-rmse:0.07266\teval-rmse:0.123065\ttrain-rmspe:0.078735\teval-rmspe:0.132769\n",
      "[664]\ttrain-rmse:0.072647\teval-rmse:0.123063\ttrain-rmspe:0.078721\teval-rmspe:0.132767\n",
      "[665]\ttrain-rmse:0.072626\teval-rmse:0.123061\ttrain-rmspe:0.078676\teval-rmspe:0.132743\n",
      "[666]\ttrain-rmse:0.072609\teval-rmse:0.123089\ttrain-rmspe:0.078657\teval-rmspe:0.132764\n",
      "[667]\ttrain-rmse:0.07258\teval-rmse:0.123086\ttrain-rmspe:0.078613\teval-rmspe:0.132758\n",
      "[668]\ttrain-rmse:0.072551\teval-rmse:0.123087\ttrain-rmspe:0.07858\teval-rmspe:0.132761\n",
      "[669]\ttrain-rmse:0.072519\teval-rmse:0.123073\ttrain-rmspe:0.078542\teval-rmspe:0.132741\n",
      "[670]\ttrain-rmse:0.072488\teval-rmse:0.123064\ttrain-rmspe:0.078511\teval-rmspe:0.132733\n",
      "[671]\ttrain-rmse:0.072464\teval-rmse:0.123056\ttrain-rmspe:0.078486\teval-rmspe:0.132733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[672]\ttrain-rmse:0.072436\teval-rmse:0.123051\ttrain-rmspe:0.078445\teval-rmspe:0.132728\n",
      "[673]\ttrain-rmse:0.072399\teval-rmse:0.123066\ttrain-rmspe:0.078405\teval-rmspe:0.132749\n",
      "[674]\ttrain-rmse:0.072375\teval-rmse:0.123062\ttrain-rmspe:0.07836\teval-rmspe:0.132744\n",
      "[675]\ttrain-rmse:0.07235\teval-rmse:0.123054\ttrain-rmspe:0.078328\teval-rmspe:0.132735\n",
      "[676]\ttrain-rmse:0.072328\teval-rmse:0.123046\ttrain-rmspe:0.078297\teval-rmspe:0.132727\n",
      "[677]\ttrain-rmse:0.072294\teval-rmse:0.123044\ttrain-rmspe:0.078251\teval-rmspe:0.132725\n",
      "[678]\ttrain-rmse:0.072262\teval-rmse:0.123038\ttrain-rmspe:0.078216\teval-rmspe:0.132719\n",
      "[679]\ttrain-rmse:0.072234\teval-rmse:0.12304\ttrain-rmspe:0.078189\teval-rmspe:0.132725\n",
      "[680]\ttrain-rmse:0.072203\teval-rmse:0.123027\ttrain-rmspe:0.07815\teval-rmspe:0.132714\n",
      "[681]\ttrain-rmse:0.072172\teval-rmse:0.123015\ttrain-rmspe:0.078116\teval-rmspe:0.132703\n",
      "[682]\ttrain-rmse:0.072153\teval-rmse:0.122998\ttrain-rmspe:0.078091\teval-rmspe:0.132691\n",
      "[683]\ttrain-rmse:0.072128\teval-rmse:0.122996\ttrain-rmspe:0.078056\teval-rmspe:0.132688\n",
      "[684]\ttrain-rmse:0.072105\teval-rmse:0.123001\ttrain-rmspe:0.07799\teval-rmspe:0.132695\n",
      "[685]\ttrain-rmse:0.07208\teval-rmse:0.122991\ttrain-rmspe:0.077901\teval-rmspe:0.132679\n",
      "[686]\ttrain-rmse:0.07205\teval-rmse:0.122986\ttrain-rmspe:0.077864\teval-rmspe:0.132682\n",
      "[687]\ttrain-rmse:0.072022\teval-rmse:0.122979\ttrain-rmspe:0.07773\teval-rmspe:0.132671\n",
      "[688]\ttrain-rmse:0.071998\teval-rmse:0.122985\ttrain-rmspe:0.077701\teval-rmspe:0.13268\n",
      "[689]\ttrain-rmse:0.071967\teval-rmse:0.122981\ttrain-rmspe:0.077583\teval-rmspe:0.132675\n",
      "[690]\ttrain-rmse:0.071913\teval-rmse:0.122958\ttrain-rmspe:0.07752\teval-rmspe:0.132656\n",
      "[691]\ttrain-rmse:0.071903\teval-rmse:0.122962\ttrain-rmspe:0.07751\teval-rmspe:0.132654\n",
      "[692]\ttrain-rmse:0.071884\teval-rmse:0.122991\ttrain-rmspe:0.07749\teval-rmspe:0.132702\n",
      "[693]\ttrain-rmse:0.071856\teval-rmse:0.122991\ttrain-rmspe:0.077456\teval-rmspe:0.132707\n",
      "[694]\ttrain-rmse:0.071827\teval-rmse:0.122979\ttrain-rmspe:0.077419\teval-rmspe:0.132696\n",
      "[695]\ttrain-rmse:0.071807\teval-rmse:0.122969\ttrain-rmspe:0.077397\teval-rmspe:0.13268\n",
      "[696]\ttrain-rmse:0.071763\teval-rmse:0.122954\ttrain-rmspe:0.077348\teval-rmspe:0.132666\n",
      "[697]\ttrain-rmse:0.071727\teval-rmse:0.122958\ttrain-rmspe:0.077303\teval-rmspe:0.132664\n",
      "[698]\ttrain-rmse:0.071703\teval-rmse:0.122963\ttrain-rmspe:0.077279\teval-rmspe:0.132669\n",
      "[699]\ttrain-rmse:0.071669\teval-rmse:0.122944\ttrain-rmspe:0.077231\teval-rmspe:0.132645\n",
      "[700]\ttrain-rmse:0.071634\teval-rmse:0.122934\ttrain-rmspe:0.077178\teval-rmspe:0.132632\n",
      "[701]\ttrain-rmse:0.071612\teval-rmse:0.122927\ttrain-rmspe:0.077051\teval-rmspe:0.132624\n",
      "[702]\ttrain-rmse:0.071591\teval-rmse:0.122907\ttrain-rmspe:0.077031\teval-rmspe:0.132608\n",
      "[703]\ttrain-rmse:0.071558\teval-rmse:0.122895\ttrain-rmspe:0.076996\teval-rmspe:0.132583\n",
      "[704]\ttrain-rmse:0.071515\teval-rmse:0.122882\ttrain-rmspe:0.076948\teval-rmspe:0.132573\n",
      "[705]\ttrain-rmse:0.071495\teval-rmse:0.122882\ttrain-rmspe:0.076926\teval-rmspe:0.132569\n",
      "[706]\ttrain-rmse:0.071468\teval-rmse:0.122877\ttrain-rmspe:0.076898\teval-rmspe:0.132561\n",
      "[707]\ttrain-rmse:0.071439\teval-rmse:0.122872\ttrain-rmspe:0.076863\teval-rmspe:0.132562\n",
      "[708]\ttrain-rmse:0.071408\teval-rmse:0.122859\ttrain-rmspe:0.076831\teval-rmspe:0.132545\n",
      "[709]\ttrain-rmse:0.071383\teval-rmse:0.122849\ttrain-rmspe:0.076795\teval-rmspe:0.132531\n",
      "[710]\ttrain-rmse:0.071333\teval-rmse:0.122834\ttrain-rmspe:0.076737\teval-rmspe:0.132524\n",
      "[711]\ttrain-rmse:0.071302\teval-rmse:0.122848\ttrain-rmspe:0.076701\teval-rmspe:0.132533\n",
      "[712]\ttrain-rmse:0.071282\teval-rmse:0.122841\ttrain-rmspe:0.076679\teval-rmspe:0.13253\n",
      "[713]\ttrain-rmse:0.071256\teval-rmse:0.122835\ttrain-rmspe:0.076644\teval-rmspe:0.132524\n",
      "[714]\ttrain-rmse:0.071233\teval-rmse:0.122843\ttrain-rmspe:0.076621\teval-rmspe:0.132535\n",
      "[715]\ttrain-rmse:0.071212\teval-rmse:0.122839\ttrain-rmspe:0.076598\teval-rmspe:0.132533\n",
      "[716]\ttrain-rmse:0.071194\teval-rmse:0.122832\ttrain-rmspe:0.076579\teval-rmspe:0.132526\n",
      "[717]\ttrain-rmse:0.071184\teval-rmse:0.122836\ttrain-rmspe:0.076569\teval-rmspe:0.132532\n",
      "[718]\ttrain-rmse:0.071168\teval-rmse:0.122841\ttrain-rmspe:0.07655\teval-rmspe:0.132533\n",
      "[719]\ttrain-rmse:0.07115\teval-rmse:0.122828\ttrain-rmspe:0.076532\teval-rmspe:0.132506\n",
      "[720]\ttrain-rmse:0.07113\teval-rmse:0.122825\ttrain-rmspe:0.076509\teval-rmspe:0.132499\n",
      "[721]\ttrain-rmse:0.071096\teval-rmse:0.122808\ttrain-rmspe:0.076467\teval-rmspe:0.132489\n",
      "[722]\ttrain-rmse:0.071077\teval-rmse:0.122826\ttrain-rmspe:0.076448\teval-rmspe:0.132512\n",
      "[723]\ttrain-rmse:0.071048\teval-rmse:0.122818\ttrain-rmspe:0.076415\teval-rmspe:0.132503\n",
      "[724]\ttrain-rmse:0.071029\teval-rmse:0.122816\ttrain-rmspe:0.076397\teval-rmspe:0.132502\n",
      "[725]\ttrain-rmse:0.071003\teval-rmse:0.122809\ttrain-rmspe:0.076368\teval-rmspe:0.132495\n",
      "[726]\ttrain-rmse:0.070975\teval-rmse:0.122804\ttrain-rmspe:0.076339\teval-rmspe:0.132486\n",
      "[727]\ttrain-rmse:0.070952\teval-rmse:0.122793\ttrain-rmspe:0.07631\teval-rmspe:0.132469\n",
      "[728]\ttrain-rmse:0.07093\teval-rmse:0.122786\ttrain-rmspe:0.076284\teval-rmspe:0.132463\n",
      "[729]\ttrain-rmse:0.070917\teval-rmse:0.122778\ttrain-rmspe:0.076269\teval-rmspe:0.132456\n",
      "[730]\ttrain-rmse:0.070902\teval-rmse:0.12279\ttrain-rmspe:0.076255\teval-rmspe:0.132474\n",
      "[731]\ttrain-rmse:0.070873\teval-rmse:0.122775\ttrain-rmspe:0.07622\teval-rmspe:0.132454\n",
      "[732]\ttrain-rmse:0.070844\teval-rmse:0.122766\ttrain-rmspe:0.076174\teval-rmspe:0.132445\n",
      "[733]\ttrain-rmse:0.070814\teval-rmse:0.122765\ttrain-rmspe:0.076139\teval-rmspe:0.132447\n",
      "[734]\ttrain-rmse:0.070772\teval-rmse:0.122747\ttrain-rmspe:0.076097\teval-rmspe:0.132428\n",
      "[735]\ttrain-rmse:0.070753\teval-rmse:0.122741\ttrain-rmspe:0.076066\teval-rmspe:0.132419\n",
      "[736]\ttrain-rmse:0.07073\teval-rmse:0.122738\ttrain-rmspe:0.076038\teval-rmspe:0.132417\n",
      "[737]\ttrain-rmse:0.070715\teval-rmse:0.122722\ttrain-rmspe:0.076022\teval-rmspe:0.132391\n",
      "[738]\ttrain-rmse:0.07069\teval-rmse:0.122723\ttrain-rmspe:0.075984\teval-rmspe:0.13239\n",
      "[739]\ttrain-rmse:0.070655\teval-rmse:0.12272\ttrain-rmspe:0.075939\teval-rmspe:0.13239\n",
      "[740]\ttrain-rmse:0.070643\teval-rmse:0.122713\ttrain-rmspe:0.075927\teval-rmspe:0.132381\n",
      "[741]\ttrain-rmse:0.070618\teval-rmse:0.122721\ttrain-rmspe:0.075895\teval-rmspe:0.13239\n",
      "[742]\ttrain-rmse:0.070599\teval-rmse:0.12272\ttrain-rmspe:0.075874\teval-rmspe:0.132387\n",
      "[743]\ttrain-rmse:0.070575\teval-rmse:0.122727\ttrain-rmspe:0.075852\teval-rmspe:0.132396\n",
      "[744]\ttrain-rmse:0.070559\teval-rmse:0.122722\ttrain-rmspe:0.075835\teval-rmspe:0.132401\n",
      "[745]\ttrain-rmse:0.070529\teval-rmse:0.122717\ttrain-rmspe:0.075805\teval-rmspe:0.132392\n",
      "[746]\ttrain-rmse:0.070509\teval-rmse:0.122718\ttrain-rmspe:0.075773\teval-rmspe:0.132395\n",
      "[747]\ttrain-rmse:0.070491\teval-rmse:0.122717\ttrain-rmspe:0.075746\teval-rmspe:0.132396\n",
      "[748]\ttrain-rmse:0.070472\teval-rmse:0.122714\ttrain-rmspe:0.075727\teval-rmspe:0.132394\n",
      "[749]\ttrain-rmse:0.07045\teval-rmse:0.122718\ttrain-rmspe:0.075701\teval-rmspe:0.132399\n",
      "[750]\ttrain-rmse:0.07043\teval-rmse:0.122696\ttrain-rmspe:0.075678\teval-rmspe:0.132365\n",
      "[751]\ttrain-rmse:0.070403\teval-rmse:0.122698\ttrain-rmspe:0.07564\teval-rmspe:0.132368\n",
      "[752]\ttrain-rmse:0.070385\teval-rmse:0.122695\ttrain-rmspe:0.075619\teval-rmspe:0.132366\n",
      "[753]\ttrain-rmse:0.070359\teval-rmse:0.122709\ttrain-rmspe:0.075591\teval-rmspe:0.132395\n",
      "[754]\ttrain-rmse:0.070333\teval-rmse:0.122716\ttrain-rmspe:0.075547\teval-rmspe:0.132403\n",
      "[755]\ttrain-rmse:0.070307\teval-rmse:0.12273\ttrain-rmspe:0.075508\teval-rmspe:0.132421\n",
      "[756]\ttrain-rmse:0.07028\teval-rmse:0.122723\ttrain-rmspe:0.075475\teval-rmspe:0.132412\n",
      "[757]\ttrain-rmse:0.070251\teval-rmse:0.122732\ttrain-rmspe:0.075444\teval-rmspe:0.132419\n",
      "[758]\ttrain-rmse:0.070221\teval-rmse:0.122715\ttrain-rmspe:0.075404\teval-rmspe:0.132404\n",
      "[759]\ttrain-rmse:0.070189\teval-rmse:0.122733\ttrain-rmspe:0.07537\teval-rmspe:0.13243\n",
      "[760]\ttrain-rmse:0.07017\teval-rmse:0.122732\ttrain-rmspe:0.07533\teval-rmspe:0.132425\n",
      "[761]\ttrain-rmse:0.070161\teval-rmse:0.122729\ttrain-rmspe:0.075319\teval-rmspe:0.132421\n",
      "[762]\ttrain-rmse:0.070132\teval-rmse:0.122716\ttrain-rmspe:0.075276\teval-rmspe:0.132404\n",
      "[763]\ttrain-rmse:0.070109\teval-rmse:0.122709\ttrain-rmspe:0.075251\teval-rmspe:0.132395\n",
      "[764]\ttrain-rmse:0.070086\teval-rmse:0.122709\ttrain-rmspe:0.075221\teval-rmspe:0.132398\n",
      "[765]\ttrain-rmse:0.070068\teval-rmse:0.122714\ttrain-rmspe:0.075201\teval-rmspe:0.132402\n",
      "[766]\ttrain-rmse:0.070035\teval-rmse:0.122726\ttrain-rmspe:0.075155\teval-rmspe:0.132419\n",
      "[767]\ttrain-rmse:0.070015\teval-rmse:0.122737\ttrain-rmspe:0.075132\teval-rmspe:0.132445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[768]\ttrain-rmse:0.069986\teval-rmse:0.122737\ttrain-rmspe:0.07509\teval-rmspe:0.132444\n",
      "[769]\ttrain-rmse:0.069952\teval-rmse:0.12273\ttrain-rmspe:0.075054\teval-rmspe:0.13244\n",
      "[770]\ttrain-rmse:0.069927\teval-rmse:0.12273\ttrain-rmspe:0.075022\teval-rmspe:0.132438\n",
      "[771]\ttrain-rmse:0.069896\teval-rmse:0.122726\ttrain-rmspe:0.074979\teval-rmspe:0.132435\n",
      "[772]\ttrain-rmse:0.069867\teval-rmse:0.122725\ttrain-rmspe:0.074947\teval-rmspe:0.132436\n",
      "[773]\ttrain-rmse:0.069839\teval-rmse:0.122728\ttrain-rmspe:0.074912\teval-rmspe:0.132438\n",
      "[774]\ttrain-rmse:0.069815\teval-rmse:0.122725\ttrain-rmspe:0.074885\teval-rmspe:0.132435\n",
      "[775]\ttrain-rmse:0.069796\teval-rmse:0.122714\ttrain-rmspe:0.074853\teval-rmspe:0.132423\n",
      "[776]\ttrain-rmse:0.069787\teval-rmse:0.122715\ttrain-rmspe:0.074844\teval-rmspe:0.132429\n",
      "[777]\ttrain-rmse:0.069754\teval-rmse:0.12271\ttrain-rmspe:0.074793\teval-rmspe:0.132421\n",
      "[778]\ttrain-rmse:0.06972\teval-rmse:0.122717\ttrain-rmspe:0.074733\teval-rmspe:0.13243\n",
      "[779]\ttrain-rmse:0.069692\teval-rmse:0.122706\ttrain-rmspe:0.074698\teval-rmspe:0.132415\n",
      "[780]\ttrain-rmse:0.069679\teval-rmse:0.122697\ttrain-rmspe:0.074685\teval-rmspe:0.132407\n",
      "[781]\ttrain-rmse:0.069659\teval-rmse:0.122701\ttrain-rmspe:0.074654\teval-rmspe:0.13241\n",
      "[782]\ttrain-rmse:0.069646\teval-rmse:0.122717\ttrain-rmspe:0.074643\teval-rmspe:0.132441\n",
      "[783]\ttrain-rmse:0.069625\teval-rmse:0.122726\ttrain-rmspe:0.074622\teval-rmspe:0.132457\n",
      "[784]\ttrain-rmse:0.069603\teval-rmse:0.122728\ttrain-rmspe:0.074598\teval-rmspe:0.13246\n",
      "[785]\ttrain-rmse:0.069573\teval-rmse:0.122717\ttrain-rmspe:0.074564\teval-rmspe:0.132441\n",
      "[786]\ttrain-rmse:0.069539\teval-rmse:0.122707\ttrain-rmspe:0.074518\teval-rmspe:0.132433\n",
      "[787]\ttrain-rmse:0.069525\teval-rmse:0.12271\ttrain-rmspe:0.074505\teval-rmspe:0.132444\n",
      "[788]\ttrain-rmse:0.069503\teval-rmse:0.122707\ttrain-rmspe:0.074481\teval-rmspe:0.132441\n",
      "[789]\ttrain-rmse:0.069485\teval-rmse:0.122707\ttrain-rmspe:0.074463\teval-rmspe:0.132439\n",
      "[790]\ttrain-rmse:0.069454\teval-rmse:0.122706\ttrain-rmspe:0.074404\teval-rmspe:0.132439\n",
      "[791]\ttrain-rmse:0.069432\teval-rmse:0.122715\ttrain-rmspe:0.074377\teval-rmspe:0.132454\n",
      "[792]\ttrain-rmse:0.069408\teval-rmse:0.122712\ttrain-rmspe:0.074345\teval-rmspe:0.132452\n",
      "[793]\ttrain-rmse:0.069385\teval-rmse:0.12272\ttrain-rmspe:0.074319\teval-rmspe:0.132467\n",
      "[794]\ttrain-rmse:0.069351\teval-rmse:0.122716\ttrain-rmspe:0.074275\teval-rmspe:0.132462\n",
      "[795]\ttrain-rmse:0.069317\teval-rmse:0.122715\ttrain-rmspe:0.074221\teval-rmspe:0.132462\n",
      "[796]\ttrain-rmse:0.06929\teval-rmse:0.122739\ttrain-rmspe:0.074185\teval-rmspe:0.132491\n",
      "[797]\ttrain-rmse:0.069269\teval-rmse:0.122735\ttrain-rmspe:0.074162\teval-rmspe:0.132486\n",
      "[798]\ttrain-rmse:0.069247\teval-rmse:0.122735\ttrain-rmspe:0.074134\teval-rmspe:0.132494\n",
      "[799]\ttrain-rmse:0.069216\teval-rmse:0.122736\ttrain-rmspe:0.074077\teval-rmspe:0.132497\n",
      "[800]\ttrain-rmse:0.06918\teval-rmse:0.12272\ttrain-rmspe:0.074037\teval-rmspe:0.132478\n",
      "[801]\ttrain-rmse:0.069144\teval-rmse:0.12271\ttrain-rmspe:0.073995\teval-rmspe:0.13247\n",
      "[802]\ttrain-rmse:0.069117\teval-rmse:0.122721\ttrain-rmspe:0.073967\teval-rmspe:0.132487\n",
      "[803]\ttrain-rmse:0.069092\teval-rmse:0.122722\ttrain-rmspe:0.07393\teval-rmspe:0.132481\n",
      "[804]\ttrain-rmse:0.069075\teval-rmse:0.122722\ttrain-rmspe:0.073871\teval-rmspe:0.132476\n",
      "[805]\ttrain-rmse:0.069056\teval-rmse:0.122722\ttrain-rmspe:0.073852\teval-rmspe:0.132478\n",
      "[806]\ttrain-rmse:0.069036\teval-rmse:0.122726\ttrain-rmspe:0.07383\teval-rmspe:0.132486\n",
      "[807]\ttrain-rmse:0.069022\teval-rmse:0.122732\ttrain-rmspe:0.07381\teval-rmspe:0.132499\n",
      "[808]\ttrain-rmse:0.069015\teval-rmse:0.122731\ttrain-rmspe:0.073804\teval-rmspe:0.132498\n",
      "[809]\ttrain-rmse:0.068996\teval-rmse:0.122718\ttrain-rmspe:0.073777\teval-rmspe:0.132477\n",
      "[810]\ttrain-rmse:0.068963\teval-rmse:0.122789\ttrain-rmspe:0.073739\teval-rmspe:0.132542\n",
      "[811]\ttrain-rmse:0.068942\teval-rmse:0.12279\ttrain-rmspe:0.073716\teval-rmspe:0.132541\n",
      "[812]\ttrain-rmse:0.068922\teval-rmse:0.122802\ttrain-rmspe:0.07369\teval-rmspe:0.132562\n",
      "[813]\ttrain-rmse:0.068901\teval-rmse:0.122791\ttrain-rmspe:0.073668\teval-rmspe:0.132548\n",
      "[814]\ttrain-rmse:0.068875\teval-rmse:0.12279\ttrain-rmspe:0.073629\teval-rmspe:0.132545\n",
      "[815]\ttrain-rmse:0.068854\teval-rmse:0.122811\ttrain-rmspe:0.073601\teval-rmspe:0.132566\n",
      "[816]\ttrain-rmse:0.068835\teval-rmse:0.122814\ttrain-rmspe:0.073574\teval-rmspe:0.132571\n",
      "[817]\ttrain-rmse:0.068824\teval-rmse:0.122819\ttrain-rmspe:0.073563\teval-rmspe:0.132576\n",
      "[818]\ttrain-rmse:0.068817\teval-rmse:0.122816\ttrain-rmspe:0.07355\teval-rmspe:0.132571\n",
      "[819]\ttrain-rmse:0.068798\teval-rmse:0.122811\ttrain-rmspe:0.073525\teval-rmspe:0.132564\n",
      "[820]\ttrain-rmse:0.068775\teval-rmse:0.122795\ttrain-rmspe:0.073498\teval-rmspe:0.132544\n",
      "[821]\ttrain-rmse:0.068746\teval-rmse:0.122806\ttrain-rmspe:0.073455\teval-rmspe:0.132559\n",
      "[822]\ttrain-rmse:0.068713\teval-rmse:0.122819\ttrain-rmspe:0.073402\teval-rmspe:0.132578\n",
      "[823]\ttrain-rmse:0.068696\teval-rmse:0.122816\ttrain-rmspe:0.073379\teval-rmspe:0.132574\n",
      "[824]\ttrain-rmse:0.068668\teval-rmse:0.122819\ttrain-rmspe:0.073343\teval-rmspe:0.13258\n",
      "[825]\ttrain-rmse:0.068658\teval-rmse:0.122822\ttrain-rmspe:0.073333\teval-rmspe:0.132586\n",
      "[826]\ttrain-rmse:0.068629\teval-rmse:0.122801\ttrain-rmspe:0.073301\teval-rmspe:0.132565\n",
      "[827]\ttrain-rmse:0.068605\teval-rmse:0.122798\ttrain-rmspe:0.073269\teval-rmspe:0.132561\n",
      "[828]\ttrain-rmse:0.068584\teval-rmse:0.122793\ttrain-rmspe:0.073246\teval-rmspe:0.132552\n",
      "[829]\ttrain-rmse:0.068557\teval-rmse:0.122803\ttrain-rmspe:0.07321\teval-rmspe:0.132564\n",
      "[830]\ttrain-rmse:0.068523\teval-rmse:0.122793\ttrain-rmspe:0.073171\teval-rmspe:0.132557\n",
      "[831]\ttrain-rmse:0.06849\teval-rmse:0.122785\ttrain-rmspe:0.073125\teval-rmspe:0.132552\n",
      "[832]\ttrain-rmse:0.06847\teval-rmse:0.12278\ttrain-rmspe:0.073101\teval-rmspe:0.132549\n",
      "[833]\ttrain-rmse:0.068456\teval-rmse:0.122788\ttrain-rmspe:0.07308\teval-rmspe:0.132561\n",
      "[834]\ttrain-rmse:0.068434\teval-rmse:0.122812\ttrain-rmspe:0.073057\teval-rmspe:0.132597\n",
      "[835]\ttrain-rmse:0.068406\teval-rmse:0.122812\ttrain-rmspe:0.073007\teval-rmspe:0.132598\n",
      "[836]\ttrain-rmse:0.068384\teval-rmse:0.122816\ttrain-rmspe:0.072978\teval-rmspe:0.132605\n",
      "[837]\ttrain-rmse:0.068355\teval-rmse:0.122814\ttrain-rmspe:0.072929\teval-rmspe:0.132604\n",
      "[838]\ttrain-rmse:0.068332\teval-rmse:0.122839\ttrain-rmspe:0.0729\teval-rmspe:0.132651\n",
      "[839]\ttrain-rmse:0.068319\teval-rmse:0.12284\ttrain-rmspe:0.072884\teval-rmspe:0.132655\n",
      "[840]\ttrain-rmse:0.068294\teval-rmse:0.12284\ttrain-rmspe:0.072855\teval-rmspe:0.132656\n",
      "[841]\ttrain-rmse:0.068281\teval-rmse:0.122851\ttrain-rmspe:0.072838\teval-rmspe:0.132672\n",
      "[842]\ttrain-rmse:0.068249\teval-rmse:0.122852\ttrain-rmspe:0.072798\teval-rmspe:0.132676\n",
      "[843]\ttrain-rmse:0.068224\teval-rmse:0.122854\ttrain-rmspe:0.072773\teval-rmspe:0.132683\n",
      "[844]\ttrain-rmse:0.068198\teval-rmse:0.122862\ttrain-rmspe:0.072726\teval-rmspe:0.132701\n",
      "[845]\ttrain-rmse:0.068182\teval-rmse:0.122864\ttrain-rmspe:0.07271\teval-rmspe:0.132701\n",
      "[846]\ttrain-rmse:0.068151\teval-rmse:0.122856\ttrain-rmspe:0.072669\teval-rmspe:0.132692\n",
      "[847]\ttrain-rmse:0.068132\teval-rmse:0.122865\ttrain-rmspe:0.072649\teval-rmspe:0.132708\n",
      "[848]\ttrain-rmse:0.068109\teval-rmse:0.122861\ttrain-rmspe:0.072622\teval-rmspe:0.132701\n",
      "[849]\ttrain-rmse:0.068082\teval-rmse:0.122861\ttrain-rmspe:0.072571\teval-rmspe:0.132698\n",
      "[850]\ttrain-rmse:0.068069\teval-rmse:0.122869\ttrain-rmspe:0.072558\teval-rmspe:0.132708\n",
      "Stopping. Best iteration:\n",
      "[750]\ttrain-rmse:0.07043\teval-rmse:0.122696\ttrain-rmspe:0.075678\teval-rmspe:0.132365\n",
      "\n",
      "time is 133.977782 s.\n",
      "testing...\n",
      "RMSPE: 0.132708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.13270790074451297, <xgboost.core.Booster at 0x2eab27ca388>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#基础模型\n",
    "train_model(params, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调试max_depth和min_child_weight\n",
    "param_grid = {'max_depth':range(9,13,1), 'min_child_weight':range(1,5,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter nums is 16.000000.\n",
      "Iter No is 1.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.8, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 138.543119 s.\n",
      "testing...\n",
      "RMSPE: 0.134324\n",
      "Iter No is 2.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 2, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.8, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 93.616510 s.\n",
      "testing...\n",
      "RMSPE: 0.136606\n",
      "Iter No is 3.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 3, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.8, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 111.664026 s.\n",
      "testing...\n",
      "RMSPE: 0.135904\n",
      "Iter No is 4.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 9, 'min_child_weight': 4, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.8, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 83.460844 s.\n",
      "testing...\n",
      "RMSPE: 0.131202\n",
      "Iter No is 5.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.8, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 140.418312 s.\n",
      "testing...\n",
      "RMSPE: 0.134813\n",
      "Iter No is 6.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 2, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.8, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 158.238012 s.\n",
      "testing...\n",
      "RMSPE: 0.137132\n",
      "Iter No is 7.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 3, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.8, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 152.463959 s.\n",
      "testing...\n",
      "RMSPE: 0.134405\n",
      "Iter No is 8.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 10, 'min_child_weight': 4, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.8, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 133.887040 s.\n",
      "testing...\n",
      "RMSPE: 0.137552\n",
      "Iter No is 9.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.8, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 172.769120 s.\n",
      "testing...\n",
      "RMSPE: 0.129999\n",
      "Iter No is 10.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 2, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.8, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 180.193924 s.\n",
      "testing...\n",
      "RMSPE: 0.134642\n",
      "Iter No is 11.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 3, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.8, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 182.872931 s.\n",
      "testing...\n",
      "RMSPE: 0.139847\n",
      "Iter No is 12.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 4, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.8, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 167.966687 s.\n",
      "testing...\n",
      "RMSPE: 0.136423\n",
      "Iter No is 13.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 12, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.8, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 233.203104 s.\n",
      "testing...\n",
      "RMSPE: 0.136078\n",
      "Iter No is 14.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 12, 'min_child_weight': 2, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.8, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 213.902998 s.\n",
      "testing...\n",
      "RMSPE: 0.132759\n",
      "Iter No is 15.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 12, 'min_child_weight': 3, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.8, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 209.871579 s.\n",
      "testing...\n",
      "RMSPE: 0.134632\n",
      "Iter No is 16.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 12, 'min_child_weight': 4, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.8, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 229.898491 s.\n",
      "testing...\n",
      "RMSPE: 0.135396\n",
      "best_param:\n",
      "{'max_depth': 11, 'min_child_weight': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xgboost.core.Booster at 0x2eab27d6f48>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_xg(params, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调试Gamma\n",
    "param_grid = {'gamma':[i/10.0 for i in range(0,10,2)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter nums is 5.000000.\n",
      "Iter No is 1.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.0, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 173.579716 s.\n",
      "testing...\n",
      "RMSPE: 0.133720\n",
      "Iter No is 2.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 203.449251 s.\n",
      "testing...\n",
      "RMSPE: 0.132894\n",
      "Iter No is 3.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.4, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 233.509041 s.\n",
      "testing...\n",
      "RMSPE: 0.138107\n",
      "Iter No is 4.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.6, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 142.871418 s.\n",
      "testing...\n",
      "RMSPE: 0.137490\n",
      "Iter No is 5.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.7, 'gamma': 0.8, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 194.324092 s.\n",
      "testing...\n",
      "RMSPE: 0.138291\n",
      "best_param:\n",
      "{'gamma': 0.2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xgboost.core.Booster at 0x2eab27cb848>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_xg(params, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调试subsample与colsample_bytree\n",
    "param_grid = {'subsample':[i/100.0 for i in range(75,96,5)],'colsample_bytree':[i/100.0 for i in range(75,96,5)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter nums is 25.000000.\n",
      "Iter No is 1.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.75, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 198.438040 s.\n",
      "testing...\n",
      "RMSPE: 0.130951\n",
      "Iter No is 2.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.75, 'colsample_bytree': 0.8, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 169.980858 s.\n",
      "testing...\n",
      "RMSPE: 0.135903\n",
      "Iter No is 3.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.75, 'colsample_bytree': 0.85, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 214.469355 s.\n",
      "testing...\n",
      "RMSPE: 0.141155\n",
      "Iter No is 4.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.75, 'colsample_bytree': 0.9, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 145.214231 s.\n",
      "testing...\n",
      "RMSPE: 0.137091\n",
      "Iter No is 5.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.75, 'colsample_bytree': 0.95, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 158.000606 s.\n",
      "testing...\n",
      "RMSPE: 0.138029\n",
      "Iter No is 6.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 207.784154 s.\n",
      "testing...\n",
      "RMSPE: 0.131308\n",
      "Iter No is 7.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 172.134988 s.\n",
      "testing...\n",
      "RMSPE: 0.134867\n",
      "Iter No is 8.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 0.85, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 151.567273 s.\n",
      "testing...\n",
      "RMSPE: 0.130584\n",
      "Iter No is 9.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 0.9, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 181.176286 s.\n",
      "testing...\n",
      "RMSPE: 0.135771\n",
      "Iter No is 10.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 0.95, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 167.417707 s.\n",
      "testing...\n",
      "RMSPE: 0.146328\n",
      "Iter No is 11.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.85, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 186.688418 s.\n",
      "testing...\n",
      "RMSPE: 0.134492\n",
      "Iter No is 12.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.85, 'colsample_bytree': 0.8, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 144.203444 s.\n",
      "testing...\n",
      "RMSPE: 0.141292\n",
      "Iter No is 13.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.85, 'colsample_bytree': 0.85, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 140.689428 s.\n",
      "testing...\n",
      "RMSPE: 0.134311\n",
      "Iter No is 14.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.85, 'colsample_bytree': 0.9, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 188.912522 s.\n",
      "testing...\n",
      "RMSPE: 0.131681\n",
      "Iter No is 15.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.85, 'colsample_bytree': 0.95, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 189.700939 s.\n",
      "testing...\n",
      "RMSPE: 0.133625\n",
      "Iter No is 16.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 195.222806 s.\n",
      "testing...\n",
      "RMSPE: 0.133639\n",
      "Iter No is 17.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.8, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 138.210831 s.\n",
      "testing...\n",
      "RMSPE: 0.132231\n",
      "Iter No is 18.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.85, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 140.772804 s.\n",
      "testing...\n",
      "RMSPE: 0.130125\n",
      "Iter No is 19.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.9, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 176.207269 s.\n",
      "testing...\n",
      "RMSPE: 0.136380\n",
      "Iter No is 20.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.95, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 172.363354 s.\n",
      "testing...\n",
      "RMSPE: 0.133590\n",
      "Iter No is 21.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 171.337287 s.\n",
      "testing...\n",
      "RMSPE: 0.128232\n",
      "Iter No is 22.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.8, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 148.278930 s.\n",
      "testing...\n",
      "RMSPE: 0.136808\n",
      "Iter No is 23.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.85, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 166.744916 s.\n",
      "testing...\n",
      "RMSPE: 0.137017\n",
      "Iter No is 24.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.9, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 148.263598 s.\n",
      "testing...\n",
      "RMSPE: 0.143958\n",
      "Iter No is 25.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.95, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
      "Start Train...\n",
      "time is 136.306579 s.\n",
      "testing...\n",
      "RMSPE: 0.136141\n",
      "best_param:\n",
      "{'subsample': 0.95, 'colsample_bytree': 0.75}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xgboost.core.Booster at 0x2eab27d0408>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_xg(params, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调试reg_alpha\n",
    "param_grid = {'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter nums is 5.000000.\n",
      "Iter No is 1.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'reg_alpha': 1e-05}\n",
      "Start Train...\n",
      "time is 193.041475 s.\n",
      "testing...\n",
      "RMSPE: 0.129778\n",
      "Iter No is 2.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'reg_alpha': 0.01}\n",
      "Start Train...\n",
      "time is 216.621211 s.\n",
      "testing...\n",
      "RMSPE: 0.130957\n",
      "Iter No is 3.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'reg_alpha': 0.1}\n",
      "Start Train...\n",
      "time is 211.673425 s.\n",
      "testing...\n",
      "RMSPE: 0.131514\n",
      "Iter No is 4.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'reg_alpha': 1}\n",
      "Start Train...\n",
      "time is 235.324176 s.\n",
      "testing...\n",
      "RMSPE: 0.131729\n",
      "Iter No is 5.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'reg_alpha': 100}\n",
      "Start Train...\n",
      "time is 122.536152 s.\n",
      "testing...\n",
      "RMSPE: 0.147894\n",
      "best_param:\n",
      "{'reg_alpha': 1e-05}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xgboost.core.Booster at 0x2eae4ba79c8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_xg(params, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调试reg_lambda\n",
    "param_grid = {'reg_lambda':[1e-5, 1e-2, 0.1, 1, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter nums is 5.000000.\n",
      "Iter No is 1.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'reg_alpha': 1e-05, 'reg_lambda': 1e-05}\n",
      "Start Train...\n",
      "time is 165.912074 s.\n",
      "testing...\n",
      "RMSPE: 0.133304\n",
      "Iter No is 2.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'reg_alpha': 1e-05, 'reg_lambda': 0.01}\n",
      "Start Train...\n",
      "time is 146.330686 s.\n",
      "testing...\n",
      "RMSPE: 0.132492\n",
      "Iter No is 3.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'reg_alpha': 1e-05, 'reg_lambda': 0.1}\n",
      "Start Train...\n",
      "time is 145.520773 s.\n",
      "testing...\n",
      "RMSPE: 0.133624\n",
      "Iter No is 4.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'reg_alpha': 1e-05, 'reg_lambda': 1}\n",
      "Start Train...\n",
      "time is 194.249824 s.\n",
      "testing...\n",
      "RMSPE: 0.130244\n",
      "Iter No is 5.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'reg_alpha': 1e-05, 'reg_lambda': 100}\n",
      "Start Train...\n",
      "time is 294.463172 s.\n",
      "testing...\n",
      "RMSPE: 0.128006\n",
      "best_param:\n",
      "{'reg_lambda': 100}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xgboost.core.Booster at 0x2eab27c5a48>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_xg(params, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调试learning_rate\n",
    "param_grid = {'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1, 0.2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter nums is 6.000000.\n",
      "Iter No is 1.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.01, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'reg_alpha': 1e-05, 'reg_lambda': 100}\n",
      "Start Train...\n",
      "time is 2053.560050 s.\n",
      "testing...\n",
      "RMSPE: 0.125774\n",
      "Iter No is 2.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.03, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'reg_alpha': 1e-05, 'reg_lambda': 100}\n",
      "Start Train...\n",
      "time is 1069.953328 s.\n",
      "testing...\n",
      "RMSPE: 0.122620\n",
      "Iter No is 3.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.05, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'reg_alpha': 1e-05, 'reg_lambda': 100}\n",
      "Start Train...\n",
      "time is 688.837221 s.\n",
      "testing...\n",
      "RMSPE: 0.127074\n",
      "Iter No is 4.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.07, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'reg_alpha': 1e-05, 'reg_lambda': 100}\n",
      "Start Train...\n",
      "time is 410.504986 s.\n",
      "testing...\n",
      "RMSPE: 0.126478\n",
      "Iter No is 5.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.1, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'reg_alpha': 1e-05, 'reg_lambda': 100}\n",
      "Start Train...\n",
      "time is 342.495050 s.\n",
      "testing...\n",
      "RMSPE: 0.127276\n",
      "Iter No is 6.000000.\n",
      "{'objective': 'reg:squarederror', 'learning_rate': 0.2, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'reg_alpha': 1e-05, 'reg_lambda': 100}\n",
      "Start Train...\n",
      "time is 263.130418 s.\n",
      "testing...\n",
      "RMSPE: 0.126263\n",
      "best_param:\n",
      "{'learning_rate': 0.03}\n"
     ]
    }
   ],
   "source": [
    "#最终模型\n",
    "gbm = grid_search_xg(params, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'reg:squarederror', 'learning_rate': 0.03, 'booster': 'gbtree', 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.95, 'colsample_bytree': 0.75, 'gamma': 0.2, 'silent': 1, 'seed': 10, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'reg_alpha': 1e-05, 'reg_lambda': 100}\n"
     ]
    }
   ],
   "source": [
    "#最终参数\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一次预测\n",
    "print(\"First predictions on the test set\")\n",
    "dtest = xgb.DMatrix(xtest)\n",
    "test_probs = gbm.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导出第一次预测结果\n",
    "result = pd.DataFrame({\"Id\": test['Id'], 'Sales': np.expm1(test_probs)})\n",
    "result.to_csv(\"Rossmann_submission_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#采用保留数据集进行检测\n",
    "print(\"validating\")\n",
    "tai_xtest.sort_index(inplace=True) \n",
    "tai_ytest.sort_index(inplace=True) \n",
    "yhat = gbm.predict(xgb.DMatrix(tai_xtest))\n",
    "error = rmspe(np.expm1(tai_ytest), np.expm1(yhat))\n",
    "\n",
    "print('RMSPE: {:.6f}'.format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis by hold-out set\n",
    "res = pd.DataFrame(data = ho_ytest)\n",
    "res['Prediction']=yhat\n",
    "res = pd.merge(ho_xtest,res, left_index= True, right_index=True)\n",
    "res['Ratio'] = res.Prediction/res.Sales\n",
    "res['Error'] =abs(res.Ratio-1)\n",
    "res['Weight'] = res.Sales/res.Prediction\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_1 = ['Sales','Prediction']\n",
    "col_2 = ['Ratio']\n",
    "L=np.random.randint( low=1,high = 1115, size = 3 ) \n",
    "print('Mean Ratio of predition and real sales data is {}: store all'.format(res.Ratio.mean()))\n",
    "for i in L:\n",
    "    s1 = pd.DataFrame(res[res['Store']==i],columns = col_1)\n",
    "    s2 = pd.DataFrame(res[res['Store']==i],columns = col_2)\n",
    "    s1.plot(title = 'Comparation of predition and real sales data: store {}'.format(i),figsize=(12,4))\n",
    "    s2.plot(title = 'Ratio of predition and real sales data: store {}'.format(i),figsize=(12,4))\n",
    "    print('Mean Ratio of predition and real sales data is {}: store {}'.format(s2.Ratio.mean(),i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.sort_values(['Error'],ascending=False,inplace= True)\n",
    "res[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole correction\n",
    "print(\"weight correction\")\n",
    "W=[(0.990+(i/1000)) for i in range(20)]\n",
    "S =[]\n",
    "for w in W:\n",
    "    error = rmspe(np.expm1(ho_ytest), np.expm1(yhat*w))\n",
    "    print('RMSPE for {:.3f}:{:.6f}'.format(w,error))\n",
    "    S.append(error)\n",
    "Score = pd.Series(S,index=W)\n",
    "Score.plot()\n",
    "BS = Score[Score.values == Score.values.min()]\n",
    "print ('Best weight for Score:{}'.format(BS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correction by store\n",
    "L=range(1115)\n",
    "W_ho=[]\n",
    "W_test=[]\n",
    "for i in L:\n",
    "    s1 = pd.DataFrame(res[res['Store']==i+1],columns = col_1)\n",
    "    s2 = pd.DataFrame(xtest[xtest['Store']==i+1])\n",
    "    W1=[(0.990+(i/1000)) for i in range(20)]\n",
    "    S =[]\n",
    "    for w in W1:\n",
    "        error = rmspe(np.expm1(s1.Sales), np.expm1(s1.Prediction*w))\n",
    "        S.append(error)\n",
    "    Score = pd.Series(S,index=W1)\n",
    "    BS = Score[Score.values == Score.values.min()]\n",
    "    a=np.array(BS.index.values)\n",
    "    b_ho=a.repeat(len(s1))\n",
    "    b_test=a.repeat(len(s2))\n",
    "    W_ho.extend(b_ho.tolist())\n",
    "    W_test.extend(b_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_new = yhat*W_ho\n",
    "error = rmspe(np.expm1(ho_ytest), np.expm1(yhat_new))\n",
    "print ('RMSPE for weight corretion {:6f}'.format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Make predictions on the test set\")\n",
    "dtest = xgb.DMatrix(xtest)\n",
    "test_probs = gbm.predict(dtest)\n",
    "\n",
    "# model1  kaggle private score 0.12647\n",
    "result = pd.DataFrame({\"Id\": test['Id'], 'Sales': np.expm1(test_probs)})\n",
    "result.to_csv(\"Rossmann_submission_1.csv\", index=False)\n",
    "\n",
    "# model2 kaggle private score 0.11756\n",
    "result = pd.DataFrame({\"Id\": test['Id'], 'Sales': np.expm1(test_probs*0.995)})\n",
    "result.to_csv(\"Rossmann_submission_2.csv\", index=False)\n",
    "\n",
    "# model3 kaggle private score 0.11292\n",
    "result = pd.DataFrame({\"Id\": test['Id'], 'Sales': np.expm1(test_probs*W_test)})\n",
    "result.to_csv(\"Rossmann_submission_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 kaggle private score 0.11756\n",
    "result = pd.DataFrame({\"Id\": test['Id'], 'Sales': np.expm1(test_probs*0.995)})\n",
    "result.to_csv(\"Rossmann_submission_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model3 kaggle private score 0.11292\n",
    "result = pd.DataFrame({\"Id\": test['Id'], 'Sales': np.expm1(test_probs*W_test)})\n",
    "result.to_csv(\"Rossmann_submission_3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
